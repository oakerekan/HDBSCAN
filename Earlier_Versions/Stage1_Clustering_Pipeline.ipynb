{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722d6a7a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Stage 1: Bottleneck Identification Notebook\n",
    "------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e73e1",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "In this first step, we load all necessary libraries and modules:\n",
    "\n",
    "- **pandas** (`pd`): for data loading and manipulation.\n",
    "- **numpy** (`np`): for numerical operations and array handling.\n",
    "- **matplotlib.pyplot** (`plt`): for creating static visualizations.\n",
    "- **seaborn** (`sns`): for enhanced statistical graphics.\n",
    "- **scipy.cluster.hierarchy** (`linkage`, `dendrogram`): for hierarchical clustering and dendrogram plotting.\n",
    "- **sklearn.preprocessing** (`StandardScaler`): for feature scaling prior to distance calculations.\n",
    "- **sklearn.cluster** (`AgglomerativeClustering`, `DBSCAN`, `KMeans`):\n",
    "  - `AgglomerativeClustering`: hierarchical clustering method.\n",
    "  - `DBSCAN`: density-based clustering fallback.\n",
    "  - `KMeans`: for inertia-based elbow analysis.\n",
    "- **sklearn.metrics** (`silhouette_score`, `silhouette_samples`, `davies_bouldin_score`, `calinski_harabasz_score`, `pairwise_distances`):\n",
    "  - Cluster validation metrics and distance computations.\n",
    "\n",
    "We also attempt to import **tslearn.metrics** for Dynamic Time Warping (DTW); if unavailable, we proceed without DTW functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, silhouette_samples,\n",
    "    davies_bouldin_score, calinski_harabasz_score,\n",
    "    pairwise_distances\n",
    ")\n",
    "\n",
    "# Try DTW\n",
    "try:\n",
    "    from tslearn.metrics import cdist_dtw\n",
    "    DTW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DTW_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905b4ac",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning\n",
    "In this section, we:\n",
    "\n",
    "1. **Load** the raw Maggi production dataset from an Excel file, parsing the start and end date‑time columns.\n",
    "2. **Verify** that all essential columns are present; raise an error if any are missing.\n",
    "3. **Clean** the data by dropping rows missing the key \"Bottleneck Duration Seconds\" value.\n",
    "4. **Classify** each event as active (`1`) or non‑active (`0`) based on the \"Stoppage Category\".\n",
    "\n",
    "- `FILEPATH`: Path to your Excel file.\n",
    "- `pd.read_excel`: Loads and parses the specified date columns.\n",
    "- `required` check: Ensures critical fields exist before proceeding.\n",
    "- `df.dropna`: Removes incomplete duration records.\n",
    "- `df['is_active']`: Creates a binary flag for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69099a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loading & Cleaning\n",
    "FILEPATH = r\"C:\\Users\\pbrin\\Downloads\\DATA WIP SORTED BASED ON ACTIVE STATE.xlsx\"\n",
    "df = pd.read_excel(FILEPATH, parse_dates=['Start Datetime','End Datetime'])\n",
    "\n",
    "# Ensure essential columns\n",
    "required = ['Line','Stoppage Category','Stoppage Reason','Shift Id','Bottleneck Duration Seconds']\n",
    "for c in required:\n",
    "    if c not in df.columns:\n",
    "        raise KeyError(f\"Missing column: {c}\")\n",
    "\n",
    "# Drop missing durations\n",
    "df = df.dropna(subset=['Bottleneck Duration Seconds'])\n",
    "\n",
    "# Classify active event\n",
    "df['is_active'] = (df['Stoppage Category']!='Not Occupied').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464028e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Advanced Preprocessing\n",
    "\n",
    "Before clustering, we apply the following steps to improve data quality:\n",
    "\n",
    "1. **Outlier Removal (IQR Method)**:\n",
    "   - Compute the 1st (Q1) and 3rd (Q3) quartiles of the \"Bottleneck Duration Seconds\".\n",
    "   - Calculate the interquartile range (IQR = Q3 − Q1).\n",
    "   - Retain only those observations within [Q1 − 1.5×IQR, Q3 + 1.5×IQR], eliminating extreme values that could skew clustering.\n",
    "\n",
    "2. **Optional Smoothing (Rolling Median)**:\n",
    "   - For time-series stability, you can smooth each line’s bottleneck durations using a rolling median (window size = 5).\n",
    "   - This reduces noise and highlights sustained bottleneck patterns.\n",
    "\n",
    "These preprocessing steps help ensure clusters reflect meaningful operational behaviors rather than artifacts or one-off spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73824ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Advanced Preprocessing\n",
    "# Remove extreme outliers using IQR\n",
    "Q1 = df['Bottleneck Duration Seconds'].quantile(0.25)\n",
    "Q3 = df['Bottleneck Duration Seconds'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask = df['Bottleneck Duration Seconds'].between(Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
    "df = df[mask]\n",
    "\n",
    "# (Optional) Smooth durations with rolling median per group\n",
    "# df['Bottleneck_Smooth'] = df.groupby(['Line'])['Bottleneck Duration Seconds'].transform(lambda x: x.rolling(5,center=True,min_periods=1).median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cb465",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics & Time‑Series Extraction\n",
    "\n",
    "In this step, we transform the cleaned data into two key structures:\n",
    "\n",
    "1. **Summary Statistics (`stats` DataFrame):**\n",
    "   - Groups: each unique combination of `Line`, `Stoppage Reason`, and `Shift Id`.\n",
    "   - Aggregations on `Bottleneck Duration Seconds`:\n",
    "     - `mean`, `std`, `min`, `max` for central tendency and dispersion.\n",
    "     - `count` (renamed to `length`) for the number of events in each group.\n",
    "   - The resulting `stats` DataFrame provides a compact feature set for clustering.\n",
    "\n",
    "2. **Time-Series Dictionary (`ts_dict`):**\n",
    "   - For each group with more than one event, we extract the raw sequence of bottleneck durations sorted by start time.\n",
    "   - This dictionary maps `(Line, Stoppage Reason, Shift Id)` keys to NumPy arrays of durations.\n",
    "   - These series can later be used for DTW-based distance computations or visual inspections.\n",
    "\n",
    "By combining statistical summaries and raw time-series, we enable both Euclidean and DTW clustering approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Summary & Time-Series Extraction\n",
    "# Summary stats per (Line,Reason,Shift)\n",
    "stats = df.groupby(['Line','Stoppage Reason','Shift Id'])['Bottleneck Duration Seconds'] \\\n",
    "    .agg(['mean','std','min','max','count']) \\\n",
    "    .rename(columns={'count':'length'}) \\\n",
    "    .dropna()\n",
    "\n",
    "# time-series dict for DTW if needed\n",
    "ts_dict = {key:grp.sort_values('Start Datetime')['Bottleneck Duration Seconds'].values\n",
    "           for key,grp in df.groupby(['Line','Stoppage Reason','Shift Id'])\n",
    "           if len(grp)>1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eba2c7",
   "metadata": {},
   "source": [
    "## 5. Distance Matrices\n",
    "\n",
    "In this section, we compute two complementary distance matrices to capture similarity:\n",
    "\n",
    "1. **Euclidean Distance on Summary Statistics**:\n",
    "   - Standardize the `stats` feature matrix using `StandardScaler`.\n",
    "   - Compute the pairwise Euclidean distances between all group feature vectors.\n",
    "   - Stores result in `D_euc` for use in clustering and heatmaps.\n",
    "\n",
    "2. **Dynamic Time Warping (DTW) Distance on Time-Series** (optional):\n",
    "   - If `tslearn` is installed (`DTW_AVAILABLE = True`), compute DTW distances between each raw bottleneck duration series in `ts_dict`.\n",
    "   - Stores result in `D_dtw`, allowing alignment-based clustering of temporal patterns.\n",
    "\n",
    "These distance matrices enable both shape-based (DTW) and magnitude-based (Euclidean) clustering approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76bbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use DTW distance if available\n",
    "if DTW_AVAILABLE:\n",
    "    from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "    \n",
    "    # Convert time series dict to list and scale\n",
    "    X_ts = list(ts_dict.values())\n",
    "    X_ts_scaled = TimeSeriesScalerMeanVariance().fit_transform(X_ts)\n",
    "\n",
    "    # Compute DTW distance matrix\n",
    "    D_dtw = cdist_dtw(X_ts_scaled)\n",
    "else:\n",
    "    D_dtw = None\n",
    "    print(\"DTW is not available. Install tslearn to compute DTW distances.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa0ed6",
   "metadata": {},
   "source": [
    "### 5b. Distance Matrix Heatmap\n",
    "\n",
    "Visualize the Euclidean distance matrix as a heatmap to identify blocks of similar group behaviors:\n",
    "\n",
    "- **sns.heatmap**: displays pairwise distances with a color gradient (`viridis`).\n",
    "- **Dark squares** along the diagonal indicate clusters of groups with low inter-group distances.\n",
    "- **Lighter regions** show group pairs that are dissimilar.\n",
    "\n",
    "This plot helps visually confirm the number of cohesive clusters before formal clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab048e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 5b. Distance Matrix Heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(D_euc, cmap='viridis')\n",
    "plt.title('Euclidean Distance Matrix Heatmap')\n",
    "plt.xlabel('Group Index')\n",
    "plt.ylabel('Group Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565edc9",
   "metadata": {},
   "source": [
    "### 5c. Elbow Plot (KMeans WCSS)\n",
    "The Elbow Method helps determine the optimal number of clusters by plotting the **Within-Cluster Sum of Squares (WCSS)** (also known as inertia) against different values of k:\n",
    "\n",
    "- **WCSS (inertia)**: measures the total squared distance between each point and the centroid in its cluster. Lower inertia indicates tighter clusters.\n",
    "- As k increases, inertia decreases because points are closer to their own centroids.\n",
    "- Look for the “elbow” point where the rate of decrease sharply changes—this suggests a balance between compact clusters and model simplicity.\n",
    "\n",
    "In this plot, we iterate k from 1 to 10, fit a `KMeans` model, record its `inertia_`, and visualize the curve to identify the optimal k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c942b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 5c. Elbow Plot (KMeans WCSS)\n",
    "wcss = []\n",
    "ks = range(1,11)\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "    wcss.append(km.inertia_)\n",
    "plt.figure()\n",
    "plt.plot(ks, wcss, marker='o')\n",
    "plt.title('Elbow Method: KMeans Inertia')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff26d9",
   "metadata": {},
   "source": [
    "### 5d Visualization Helpers\n",
    "\n",
    "We define reusable plotting functions to streamline our visual analysis:\n",
    "\n",
    "- **`plot_dendrogram(Z)`**: Generates a dendrogram from a linkage matrix `Z`:\n",
    "  - **`figsize=(10,5)`** ensures readability.\n",
    "  - **`color_threshold=0`** draws all clusters in a single color for clarity.\n",
    "  - Labels the axes (`Group index`, `Distance`) and titles the chart.\n",
    "\n",
    "Additional helper functions (e.g., silhouette curve, time-series plotting) can be defined here to keep the main notebook cells concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5d. Visualization helpers\n",
    "\n",
    "def plot_dendrogram(Z):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    dendrogram(Z, color_threshold=0)\n",
    "    plt.title('Dendrogram: AHC (Complete Linkage)')\n",
    "    plt.xlabel('Group index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9d656",
   "metadata": {},
   "source": [
    "## 6. Parameter Tuning for AHC\n",
    "\n",
    "In this step, we identify the optimal number of clusters (k) for Agglomerative Hierarchical Clustering (AHC) using silhouette analysis:\n",
    "\n",
    "1. **Iterate** k from 2 to 7.\n",
    "2. **Fit** an AHC model with `metric='precomputed'` and `linkage='complete'` on the Euclidean distance matrix (`D_euc`).\n",
    "3. **Compute** the average silhouette score for each k to assess cluster cohesion and separation.\n",
    "4. **Store** results in `sil_df` and **plot** silhouette vs. k to visually identify the best cluster count.\n",
    "5. **Select** `opt_k` as the k value with the highest silhouette score for final clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 6. Parameter Tuning for AHC\n",
    "sil_scores = []\n",
    "for k in range(2,8):\n",
    "    model = AgglomerativeClustering(n_clusters=k, metric='precomputed', linkage='complete')\n",
    "    labels = model.fit_predict(D_euc)\n",
    "    sil = silhouette_score(D_euc, labels, metric='precomputed')\n",
    "    sil_scores.append({'k':k,'silhouette':sil})\n",
    "sil_df = pd.DataFrame(sil_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sil_df['k'], sil_df['silhouette'], marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k\n",
    "opt_k = sil_df.loc[sil_df['silhouette'].idxmax(),'k']\n",
    "print(f\"Optimal clusters by silhouette: {opt_k}\")\n",
    "\n",
    "# 6. Density-based clustering via DBSCAN (HDBSCAN proxy)\n",
    "\n",
    "def run_dbscan(D: np.ndarray, eps: float, min_samples: int = 5):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    labels = db.fit_predict(D)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148cd4c",
   "metadata": {},
   "source": [
    "## 7. Clustering Execution\n",
    "\n",
    "With the optimal cluster count (`opt_k`) determined, we perform our final clustering methods:\n",
    "\n",
    "1. **Agglomerative Hierarchical Clustering (AHC)**\n",
    "   - Compute the linkage matrix `Z` using complete linkage on the Euclidean distance matrix (`D_euc`).\n",
    "   - Fit the AHC model with `n_clusters=opt_k`, `metric='precomputed'`, and `linkage='complete'` to generate `labels_ahc`.\n",
    "\n",
    "2. **Density-Based Clustering (HDBSCAN / DBSCAN fallback)**\n",
    "   - Attempt to import and fit **HDBSCAN** with `metric='precomputed'` and `min_cluster_size=10` to generate `labels_hdb`.\n",
    "   - If **HDBSCAN** is unavailable, fall back to **DBSCAN**, setting `eps` to the median value of `D_euc` and `min_samples=5`.\n",
    "\n",
    "These cluster labels (`labels_ahc` and `labels_hdb`) will be used in the next evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e87558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 7. Clustering Execution\n",
    "# AHC with optimal k\n",
    "Z = linkage(D_euc, method='complete')\n",
    "labels_ahc = AgglomerativeClustering(\n",
    "    n_clusters=int(opt_k), metric='precomputed', linkage='complete'\n",
    ").fit_predict(D_euc)\n",
    "\n",
    "# HDBSCAN or DBSCAN\n",
    "try:\n",
    "    import hdbscan\n",
    "    labels_hdb = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=10).fit_predict(D_euc)\n",
    "except ImportError:\n",
    "    labels_hdb = DBSCAN(eps=np.median(D_euc),min_samples=5,metric='precomputed').fit_predict(D_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83c241",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "\n",
    "We now validate cluster quality using three internal metrics:\n",
    "\n",
    "- **Silhouette Score**: Evaluates how well each data point fits within its assigned cluster vs. the next best one (−1 to +1, higher is better).\n",
    "- **Davies–Bouldin Index**: Captures average similarity between each cluster and its most similar one (lower is better).\n",
    "- **Calinski–Harabasz Index**: Reflects the ratio of between-cluster dispersion to within-cluster dispersion (higher is better).\n",
    "\n",
    "We implement `compute_metrics` to:\n",
    "1. Compute the silhouette score using the precomputed distance matrix (`D_euc`).\n",
    "2. Compute DB and CH scores on the standardized features (`X`).\n",
    "\n",
    "Then, we compare metrics for both the hierarchical clusters (`labels_ahc`) and density-based clusters (`labels_hdb`) to determine which method yields more coherent groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 8. Evaluation Metrics\n",
    "\n",
    "def compute_metrics(labels, D, X):\n",
    "    m = {}\n",
    "    m['silhouette'] = silhouette_score(D, labels, metric='precomputed') if len(set(labels))>1 else np.nan\n",
    "    mask = labels>=0\n",
    "    m['db'] = davies_bouldin_score(X[mask], labels[mask]) if len(set(labels[mask]))>1 else np.nan\n",
    "    m['ch'] = calinski_harabasz_score(X[mask], labels[mask]) if len(set(labels[mask]))>1 else np.nan\n",
    "    return m\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'AHC': compute_metrics(labels_ahc, D_euc, X),\n",
    "    'Density': compute_metrics(labels_hdb, D_euc, X)\n",
    "}).T\n",
    "\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f7a88",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "In this section, we create two primary plots to evaluate our clustering results:\n",
    "\n",
    "1. **Dendrogram (Agglomerative Clustering)**\n",
    "   - Visualizes the hierarchical merging of clusters based on linkage distance.\n",
    "   - Use it to confirm the chosen number of clusters and identify well-separated groupings.\n",
    "\n",
    "2. **Silhouette Plot (AHC)**\n",
    "   - Shows the silhouette coefficient for each group, organized by cluster.\n",
    "   - The silhouette coefficient measures how similar an object is to its own cluster compared to other clusters (range: -1 to +1).\n",
    "   - A red dashed line indicates the average silhouette score, providing a benchmark for overall clustering quality.\n",
    "\n",
    "These visualizations help you assess cluster cohesion, separation, and identify any outliers or misclassified groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 9. Visualizations\n",
    "# Dendrogram\n",
    "plt.figure(figsize=(10,5))\n",
    "_ = dendrogram(Z, color_threshold=0)\n",
    "plt.title('Dendrogram: AHC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette plot for AHC\n",
    "sample_sil = silhouette_samples(D_euc, labels_ahc, metric='precomputed')\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,4))\n",
    "ax1.set_title('Silhouette Plot: AHC')\n",
    "ax1.set_xlabel('Silhouette Coefficient')\n",
    "ax1.set_ylabel('Cluster')\n",
    "y_lower = 10\n",
    "for i in range(int(opt_k)):\n",
    "    ith_sil = sample_sil[labels_ahc==i]\n",
    "    ith_sil.sort()\n",
    "    size = ith_sil.shape[0]\n",
    "    y_upper = y_lower + size\n",
    "    ax1.fill_betweenx(np.arange(y_lower,y_upper), 0, ith_sil)\n",
    "    ax1.text(-0.05, y_lower + 0.5*size, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "ax1.axvline(x=metrics.loc['AHC','silhouette'], color=\"red\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74993841",
   "metadata": {},
   "source": [
    "## 10. Clustered Time-Series Samples\n",
    "\n",
    "To visually validate the temporal coherence within each cluster, we plot sample bottleneck duration series for the first three clusters:\n",
    "\n",
    "1. **Select** up to 5 representative time-series from each cluster (`labels_ahc`).\n",
    "2. **Overlay** these series in a single plot, using semi-transparent lines (`alpha=0.7`).\n",
    "3. **Observe** common patterns such as repeated spikes, flat periods, or consistent run-times within each cluster.\n",
    "\n",
    "This visual check ensures that clusters capture similar dynamic behaviors, confirming that the grouping makes operational sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 10. Clustered Time-Series Samples (first 3 clusters)\n",
    "for cluster_id in range(min(3,int(opt_k))):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Sample Time-Series: Cluster {cluster_id}\")\n",
    "    keys = [k for k,l in zip(ts_dict.keys(),labels_ahc) if l==cluster_id]\n",
    "    for key in keys[:5]:\n",
    "        plt.plot(ts_dict[key], alpha=0.7)\n",
    "    plt.xlabel('Event Index')\n",
    "    plt.ylabel('Duration (sec)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b63501",
   "metadata": {},
   "source": [
    "## 11. Export Results\n",
    "\n",
    "In this final step, we persist our clustering outputs for further use and reporting:\n",
    "\n",
    "1. **Annotate** the `stats` DataFrame with cluster labels:\n",
    "   - `AHC_label` for hierarchical clustering results.\n",
    "   - `Density_label` for density-based clustering results.\n",
    "2. **Export** key artifacts:\n",
    "   - **`clustering_metrics_stage1.csv`**: a CSV file containing the performance metrics for each clustering method.\n",
    "   - **`cluster_assignments_stage1.csv`**: a CSV file with the original `stats` plus cluster labels for each group.\n",
    "3. **Confirmation**: print a message indicating successful export.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 11. Export Results\n",
    "stats['AHC_label'] = labels_ahc\n",
    "stats['Density_label'] = labels_hdb\n",
    "metrics.to_csv('clustering_metrics_stage1.csv')\n",
    "stats.to_csv('cluster_assignments_stage1.csv')\n",
    "print(\"Exports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de08d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
