{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bb6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "Following research methodology: 7-module CRISP-DM inspired approach\n",
      "Adapted for stoppage reason pattern identification\n",
      "Manufacturing Stoppage Analysis - Complete Research Methodology\n",
      "======================================================================\n",
      "\n",
      "This notebook implements the complete 7-module research methodology\n",
      "from the paper: 'Generic hierarchical clustering approach to throughput\n",
      "bottleneck detection' adapted for stoppage pattern analysis.\n",
      "\n",
      "UPDATED: Now supports Excel file input (.xlsx, .xls)\n",
      "\n",
      "To execute the complete analysis:\n",
      "\n",
      "1. With your Excel data:\n",
      "   results = run_complete_research_methodology('your_data.xlsx')\n",
      "\n",
      "2. With your CSV data:\n",
      "   results = run_complete_research_methodology('your_data.csv')\n",
      "\n",
      "3. With sample data:\n",
      "   results = run_complete_research_methodology(None)\n",
      "\n",
      "4. Create visualizations:\n",
      "   create_comprehensive_visualizations(results)\n",
      "\n",
      "5. Generate report:\n",
      "   generate_research_report(results)\n",
      "\n",
      "Expected Excel columns:\n",
      "- Line (or Production Line, Machine)\n",
      "- Stoppage Reason (or Reason, Downtime Reason)\n",
      "- Start Datetime (or Start Time)\n",
      "- End Datetime (or End Time)\n",
      "- Shift Id (or Shift)\n",
      "\n",
      "Note: Column names are automatically standardized\n",
      "\n",
      "======================================================================\n",
      "Ready to execute complete research methodology with Excel support!\n",
      "\n",
      "==================================================\n",
      "EXECUTING: Running with Excel file\n",
      "==================================================\n",
      "================================================================================\n",
      "MANUFACTURING STOPPAGE ANALYSIS - COMPLETE RESEARCH METHODOLOGY\n",
      "Following 7-module CRISP-DM approach from research paper\n",
      "Adapted from throughput bottleneck detection to stoppage pattern analysis\n",
      "UPDATED: Supporting Excel file input (.xlsx, .xls)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "STARTING MODULE 1: DATA COLLECTION FROM EXCEL\n",
      "============================================================\n",
      "=== MODULE 1: DATA COLLECTION ===\n",
      "Following research methodology for event log data extraction\n",
      "UPDATED: Supporting Excel file input\n",
      "✓ Event log data extracted from Excel file: data\\Line2.xlsx\n",
      "--- Excel Column Validation ---\n",
      "Current columns in Excel: ['Line', 'Stoppage Reason', 'Start Datetime', 'End Datetime', 'Shift Id']\n",
      "✓ Columns renamed: {'Line': 'Line', 'Stoppage Reason': 'Stoppage Reason', 'Start Datetime': 'Start Datetime', 'End Datetime': 'End Datetime', 'Shift Id': 'Shift Id'}\n",
      "✓ All expected columns found: ['Line', 'Stoppage Reason', 'Start Datetime', 'End Datetime', 'Shift Id']\n",
      "--- DateTime Column Conversion ---\n",
      "✓ Successfully converted 'Start Datetime' to datetime\n",
      "✓ Successfully converted 'End Datetime' to datetime\n",
      "✓ DateTime range: 2023-12-31 20:00:00 to 2024-03-31 14:00:00\n",
      "✓ Time interval defined: 360 days\n",
      "✓ Data period: 2023-04-06 to 2024-03-31\n",
      "✓ Total events collected: 620\n",
      "\n",
      "--- Event Log Data Validation ---\n",
      "Data shape: (620, 5)\n",
      "Unique stoppage reasons: 26\n",
      "Date range: 2023-12-31 20:00:00 to 2024-03-31 14:00:00\n",
      "Lines covered: ['Line 2']\n",
      "Shifts covered: ['C' 'B' 'A']\n",
      "✓ Unevenly spaced time series confirmed (interval variance: 10166.17 seconds)\n",
      "\n",
      "============================================================\n",
      "STARTING MODULE 2: METHOD SELECTION\n",
      "============================================================\n",
      "\n",
      "=== MODULE 2: SELECTION OF SUITABLE DETECTION METHOD ===\n",
      "Following research methodology for method selection with domain expert input\n",
      "--- Domain Expert Input Collection ---\n",
      "✓ Domain expert input collected\n",
      "  Target analysis: stoppage_pattern_identification\n",
      "  Method selected: hierarchical_clustering_with_density_comparison\n",
      "--- Data Suitability Verification (Feedback Loop) ---\n",
      "✓ Data verification completed:\n",
      "  ✓ time_series_data: True\n",
      "  ✓ event_classification: True\n",
      "  ✓ data_volume: True\n",
      "  ✓ time_coverage: True\n",
      "  ✓ event_diversity: True\n",
      "✓ All data requirements met for selected method\n",
      "--- Method Selection Finalization ---\n",
      "✓ Final method selection: hierarchical_clustering_with_density_comparison\n",
      "✓ Method components:\n",
      "  - Primary: Agglomerative Hierarchical Clustering (AHC)\n",
      "  - Comparison: HDBSCAN\n",
      "✓ Rationale:\n",
      "  - AHC chosen following research methodology for hierarchical structure\n",
      "  - Complete linkage provides clear separation between stoppage patterns\n",
      "  - HDBSCAN added for density-based analysis and outlier detection\n",
      "  - Both methods suitable for event log time series from manufacturing\n",
      "✓ Method parameters configured following research specifications\n",
      "\n",
      "============================================================\n",
      "STARTING MODULE 3: DATA PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "=== MODULE 3: DATA PRE-PROCESSING ===\n",
      "Following research methodology for event log preprocessing\n",
      "Computing metrics for stoppage pattern analysis\n",
      "--- Data Cleaning (Following Research Methodology) ---\n",
      "✓ Data cleaning completed (research methodology):\n",
      "  Initial events: 620\n",
      "  Production hours filter: 530 events retained\n",
      "  Duplicates removed: 0\n",
      "  Final events: 530\n",
      "  Total removed: 90 events\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Module3_DataPreprocessing' object has no attribute '_classify_events'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2739\u001b[39m\n\u001b[32m   2736\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEXECUTING: Running with Excel file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2737\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2739\u001b[39m results = \u001b[43mrun_complete_research_methodology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2740\u001b[39m create_comprehensive_visualizations(results)\n\u001b[32m   2741\u001b[39m generate_research_report(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2140\u001b[39m, in \u001b[36mrun_complete_research_methodology\u001b[39m\u001b[34m(data_source, time_interval_days)\u001b[39m\n\u001b[32m   2137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTARTING MODULE 3: DATA PREPROCESSING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2140\u001b[39m processed_data = \u001b[43mmodule3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_event_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[38;5;66;03m# Generate time series matrix (research methodology)\u001b[39;00m\n\u001b[32m   2143\u001b[39m time_series_matrix = module3._generate_time_series_features()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 449\u001b[39m, in \u001b[36mModule3_DataPreprocessing.preprocess_event_data\u001b[39m\u001b[34m(self, raw_data)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m._clean_event_data()\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# Step 2: Event classification (domain expert definitions) - FIX THE METHOD CALL\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m event_definitions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_classify_events\u001b[49m()  \u001b[38;5;66;03m# This method needs to exist\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Step 3: Metric computation (adapted from research)\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28mself\u001b[39m._compute_stoppage_metrics()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Module3_DataPreprocessing' object has no attribute '_classify_events'"
     ]
    }
   ],
   "source": [
    "# Manufacturing Stoppage Analysis: HDBSCAN vs AHC Following Research Methodology\n",
    "# Adapted from \"Generic hierarchical clustering approach to throughput bottleneck detection\"\n",
    "# Focus: Stoppage reason pattern identification using event log data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Clustering and time series libraries\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "import hdbscan\n",
    "# Time series analysis\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(\"Following research methodology: 7-module CRISP-DM inspired approach\")\n",
    "print(\"Adapted for stoppage reason pattern identification\")\n",
    "# =============================================================================\n",
    "# MODULE 1: DATA COLLECTION\n",
    "# Following the research methodology for event log data collection\n",
    "# =============================================================================\n",
    "class Module1_DataCollection:\n",
    "   \"\"\"\n",
    "   Module 1: Data collection from manufacturing event logs\n",
    "   Adapted from research methodology for stoppage analysis\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.raw_event_data = None\n",
    "       self.time_interval_defined = False\n",
    "       self.domain_expert_input = {}\n",
    "       \n",
    "   def collect_event_log_data(self, data_source, time_interval_days=30):\n",
    "       \"\"\"\n",
    "       Collect event log data following research methodology\n",
    "       UPDATED: Support for Excel files (.xlsx, .xls)\n",
    "       \n",
    "       Parameters:\n",
    "       - data_source: Excel file path (.xlsx/.xls) or DataFrame with stoppage events\n",
    "       - time_interval_days: Historical data period (default 30 days as in research)\n",
    "       \"\"\"\n",
    "       print(\"=== MODULE 1: DATA COLLECTION ===\")\n",
    "       print(\"Following research methodology for event log data extraction\")\n",
    "       print(\"UPDATED: Supporting Excel file input\")\n",
    "       \n",
    "       if isinstance(data_source, str):\n",
    "           # Load from Excel file (simulating MES data extraction)\n",
    "           if data_source.endswith(('.xlsx', '.xls')):\n",
    "               try:\n",
    "                   self.raw_event_data = pd.read_excel(data_source)\n",
    "                   print(f\"✓ Event log data extracted from Excel file: {data_source}\")\n",
    "               except Exception as e:\n",
    "                   print(f\"✗ Error reading Excel file: {e}\")\n",
    "                   print(\"  Creating sample data for demonstration...\")\n",
    "                   self.raw_event_data = self._create_sample_event_data()\n",
    "           else:\n",
    "               # Fallback to CSV if not Excel\n",
    "               try:\n",
    "                   self.raw_event_data = pd.read_csv(data_source)\n",
    "                   print(f\"✓ Event log data extracted from CSV file: {data_source}\")\n",
    "               except Exception as e:\n",
    "                   print(f\"✗ Error reading file: {e}\")\n",
    "                   print(\"  Creating sample data for demonstration...\")\n",
    "                   self.raw_event_data = self._create_sample_event_data()\n",
    "       elif isinstance(data_source, pd.DataFrame):\n",
    "           self.raw_event_data = data_source.copy()\n",
    "           print(\"✓ Event log data loaded from provided DataFrame\")\n",
    "       else:\n",
    "           # Use sample data for demonstration\n",
    "           self.raw_event_data = self._create_sample_event_data()\n",
    "           print(\"✓ Sample event log data created for demonstration\")\n",
    "       \n",
    "       # Validate and clean column names\n",
    "       self._validate_excel_columns()\n",
    "       \n",
    "       # Convert datetime columns (following research format)\n",
    "       self._convert_datetime_columns()\n",
    "       \n",
    "       # Define time interval of interest (research methodology step)\n",
    "       self._define_time_interval(time_interval_days)\n",
    "       \n",
    "       # Filter data to time interval\n",
    "       end_date = self.raw_event_data['Start Datetime'].max()\n",
    "       start_date = end_date - timedelta(days=time_interval_days)\n",
    "       self.raw_event_data = self.raw_event_data[\n",
    "           (self.raw_event_data['Start Datetime'] >= start_date) & \n",
    "           (self.raw_event_data['Start Datetime'] <= end_date)\n",
    "       ]\n",
    "       \n",
    "       print(f\"✓ Time interval defined: {time_interval_days} days\")\n",
    "       print(f\"✓ Data period: {start_date.date()} to {end_date.date()}\")\n",
    "       print(f\"✓ Total events collected: {len(self.raw_event_data)}\")\n",
    "       \n",
    "       return self._validate_event_data()\n",
    "   \n",
    "   def _validate_excel_columns(self):\n",
    "       \"\"\"\n",
    "       Validate and standardize Excel column names\n",
    "       Handle common variations in Excel column naming\n",
    "       \"\"\"\n",
    "       print(\"--- Excel Column Validation ---\")\n",
    "       \n",
    "       # Expected columns\n",
    "       expected_columns = ['Line', 'Stoppage Reason', 'Start Datetime', 'End Datetime', 'Shift Id']\n",
    "       \n",
    "       # Common variations in Excel files\n",
    "       column_mappings = {\n",
    "           'line': 'Line',\n",
    "           'production line': 'Line',\n",
    "           'machine': 'Line',\n",
    "           'stoppage reason': 'Stoppage Reason',\n",
    "           'reason': 'Stoppage Reason',\n",
    "           'downtime reason': 'Stoppage Reason',\n",
    "           'start datetime': 'Start Datetime',\n",
    "           'start time': 'Start Datetime',\n",
    "           'start_datetime': 'Start Datetime',\n",
    "           'start_time': 'Start Datetime',\n",
    "           'end datetime': 'End Datetime',\n",
    "           'end time': 'End Datetime',\n",
    "           'end_datetime': 'End Datetime',\n",
    "           'end_time': 'End Datetime',\n",
    "           'shift id': 'Shift Id',\n",
    "           'shift': 'Shift Id',\n",
    "           'shift_id': 'Shift Id'\n",
    "       }\n",
    "       \n",
    "       # Get current columns\n",
    "       current_columns = list(self.raw_event_data.columns)\n",
    "       print(f\"Current columns in Excel: {current_columns}\")\n",
    "       \n",
    "       # Apply mappings\n",
    "       renamed_columns = {}\n",
    "       for col in current_columns:\n",
    "           col_lower = col.lower().strip()\n",
    "           if col_lower in column_mappings:\n",
    "               renamed_columns[col] = column_mappings[col_lower]\n",
    "       \n",
    "       if renamed_columns:\n",
    "           self.raw_event_data = self.raw_event_data.rename(columns=renamed_columns)\n",
    "           print(f\"✓ Columns renamed: {renamed_columns}\")\n",
    "       \n",
    "       # Check for missing required columns\n",
    "       final_columns = list(self.raw_event_data.columns)\n",
    "       missing_columns = [col for col in expected_columns if col not in final_columns]\n",
    "       \n",
    "       if missing_columns:\n",
    "           print(f\"⚠ Warning: Missing expected columns: {missing_columns}\")\n",
    "           print(\"  Available columns:\", final_columns)\n",
    "       else:\n",
    "           print(f\"✓ All expected columns found: {expected_columns}\")\n",
    "   \n",
    "   def _convert_datetime_columns(self):\n",
    "       \"\"\"\n",
    "       Convert datetime columns with robust Excel datetime handling\n",
    "       \"\"\"\n",
    "       print(\"--- DateTime Column Conversion ---\")\n",
    "       \n",
    "       datetime_columns = ['Start Datetime', 'End Datetime']\n",
    "       \n",
    "       for col in datetime_columns:\n",
    "           if col in self.raw_event_data.columns:\n",
    "               try:\n",
    "                   # Handle various Excel datetime formats\n",
    "                   self.raw_event_data[col] = pd.to_datetime(\n",
    "                       self.raw_event_data[col], \n",
    "                       infer_datetime_format=True,\n",
    "                       errors='coerce'  # Convert invalid dates to NaT\n",
    "                   )\n",
    "                   \n",
    "                   # Check for conversion issues\n",
    "                   null_count = self.raw_event_data[col].isnull().sum()\n",
    "                   if null_count > 0:\n",
    "                       print(f\"⚠ Warning: {null_count} invalid datetime values in '{col}' converted to NaT\")\n",
    "                   else:\n",
    "                       print(f\"✓ Successfully converted '{col}' to datetime\")\n",
    "                       \n",
    "               except Exception as e:\n",
    "                   print(f\"✗ Error converting '{col}' to datetime: {e}\")\n",
    "           else:\n",
    "               print(f\"⚠ Column '{col}' not found in data\")\n",
    "       \n",
    "       # Show datetime range\n",
    "       if 'Start Datetime' in self.raw_event_data.columns:\n",
    "           start_range = self.raw_event_data['Start Datetime'].min()\n",
    "           end_range = self.raw_event_data['Start Datetime'].max()\n",
    "           print(f\"✓ DateTime range: {start_range} to {end_range}\")\n",
    "   \n",
    "   def _create_sample_event_data(self):\n",
    "       \"\"\"Create sample event log data following research format\"\"\"\n",
    "       # Extended sample data for demonstration\n",
    "       sample_data = {\n",
    "           'Line': ['Line 2'] * 20,\n",
    "           'Stoppage Reason': [\n",
    "               'CIL', 'CIL', 'End of Shift Cleaning', 'Intermittent Cleaning',\n",
    "               'Intermittent Cleaning', 'Intermittent Cleaning', 'Punch Tip Checks', 'CIL',\n",
    "               'Machine Breakdown', 'Tool Change', 'Quality Check', 'Material Shortage',\n",
    "               'Operator Break', 'CIL', 'End of Shift Cleaning', 'Machine Setup',\n",
    "               'Preventive Maintenance', 'Tool Change', 'Quality Check', 'CIL'\n",
    "           ],\n",
    "           'Start Datetime': [\n",
    "               '2024-05-20 14:00:00', '2024-05-20 07:00:00', '2024-05-20 06:55:00',\n",
    "               '2024-05-20 06:00:00', '2024-05-20 03:00:00', '2024-05-20 00:00:00',\n",
    "               '2024-05-19 20:18:00', '2024-05-19 20:05:00', '2024-05-19 15:30:00',\n",
    "               '2024-05-19 10:15:00', '2024-05-19 08:45:00', '2024-05-18 16:20:00',\n",
    "               '2024-05-18 12:00:00', '2024-05-18 09:30:00', '2024-05-18 06:55:00',\n",
    "               '2024-05-17 22:10:00', '2024-05-17 18:00:00', '2024-05-17 14:25:00',\n",
    "               '2024-05-17 11:40:00', '2024-05-17 08:15:00'\n",
    "           ],\n",
    "           'End Datetime': [\n",
    "               '2024-05-20 14:34:00', '2024-05-20 07:40:00', '2024-05-20 07:00:00',\n",
    "               '2024-05-20 06:03:00', '2024-05-20 03:03:00', '2024-05-20 00:03:00',\n",
    "               '2024-05-19 20:23:00', '2024-05-19 20:18:00', '2024-05-19 16:45:00',\n",
    "               '2024-05-19 10:35:00', '2024-05-19 09:00:00', '2024-05-18 16:35:00',\n",
    "               '2024-05-18 12:30:00', '2024-05-18 09:45:00', '2024-05-18 07:00:00',\n",
    "               '2024-05-17 22:25:00', '2024-05-17 19:30:00', '2024-05-17 14:45:00',\n",
    "               '2024-05-17 12:00:00', '2024-05-17 08:30:00'\n",
    "           ],\n",
    "           'Shift Id': ['C', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'C', 'B', 'A', 'A', 'A', 'C', 'B', 'A', 'B']\n",
    "       }\n",
    "       return pd.DataFrame(sample_data)\n",
    "   \n",
    "   def _define_time_interval(self, days):\n",
    "       \"\"\"Define time interval following research domain expert input\"\"\"\n",
    "       self.domain_expert_input = {\n",
    "           'time_interval_days': days,\n",
    "           'rationale': f'Historical data of {days} days represents production system dynamics well',\n",
    "           'minimum_runs_covered': 'Enough past production runs for event representation',\n",
    "           'expert_validation': 'Domain expert confirmed adequate coverage'\n",
    "       }\n",
    "       self.time_interval_defined = True\n",
    "   \n",
    "   def _validate_event_data(self):\n",
    "       \"\"\"Validate collected event log data\"\"\"\n",
    "       print(\"\\n--- Event Log Data Validation ---\")\n",
    "       print(f\"Data shape: {self.raw_event_data.shape}\")\n",
    "       print(f\"Unique stoppage reasons: {self.raw_event_data['Stoppage Reason'].nunique()}\")\n",
    "       print(f\"Date range: {self.raw_event_data['Start Datetime'].min()} to {self.raw_event_data['Start Datetime'].max()}\")\n",
    "       print(f\"Lines covered: {self.raw_event_data['Line'].unique()}\")\n",
    "       print(f\"Shifts covered: {self.raw_event_data['Shift Id'].unique()}\")\n",
    "       \n",
    "       # Check for unevenly spaced time series (as mentioned in research)\n",
    "       time_intervals = self.raw_event_data['Start Datetime'].diff().dt.total_seconds().dropna()\n",
    "       print(f\"✓ Unevenly spaced time series confirmed (interval variance: {time_intervals.std():.2f} seconds)\")\n",
    "       \n",
    "       return self.raw_event_data\n",
    "# =============================================================================\n",
    "# MODULE 2: SELECTING SUITABLE BOTTLENECK DETECTION METHOD\n",
    "# Adapted for stoppage pattern detection method selection\n",
    "# =============================================================================\n",
    "class Module2_MethodSelection:\n",
    "   \"\"\"\n",
    "   Module 2: Selection of suitable bottleneck detection method\n",
    "   Following research methodology with domain expert input and feedback loop\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.selected_method = None\n",
    "       self.method_justification = {}\n",
    "       self.domain_expert_input = {}\n",
    "       self.data_verification = {}\n",
    "       \n",
    "   def select_detection_method(self, event_data, domain_requirements=None):\n",
    "       \"\"\"\n",
    "       Select appropriate detection method following research methodology\n",
    "       Includes domain expert decision making and data verification feedback loop\n",
    "       \"\"\"\n",
    "       print(\"\\n=== MODULE 2: SELECTION OF SUITABLE DETECTION METHOD ===\")\n",
    "       print(\"Following research methodology for method selection with domain expert input\")\n",
    "       \n",
    "       # Step 1: Domain expert decision (following research approach)\n",
    "       self._gather_domain_expert_input(domain_requirements)\n",
    "       \n",
    "       # Step 2: Verify data suitability (feedback loop with Module 1)\n",
    "       self._verify_data_suitability(event_data)\n",
    "       \n",
    "       # Step 3: Final method selection\n",
    "       self._finalize_method_selection()\n",
    "       \n",
    "       return self.selected_method\n",
    "   \n",
    "   def _gather_domain_expert_input(self, requirements):\n",
    "       \"\"\"\n",
    "       Gather domain expert input for method selection (following research)\n",
    "       \"\"\"\n",
    "       print(\"--- Domain Expert Input Collection ---\")\n",
    "       \n",
    "       # Simulate domain expert requirements (in practice, this would be actual expert input)\n",
    "       if requirements is None:\n",
    "           requirements = {\n",
    "               'target_analysis': 'stoppage_pattern_identification',\n",
    "               'practical_requirements': [\n",
    "                   'Identify similar stoppage patterns for targeted interventions',\n",
    "                   'Detect anomalous stoppages requiring special attention',\n",
    "                   'Understand hierarchical relationships between stoppage types',\n",
    "                   'Support both planned and unplanned stoppage analysis'\n",
    "               ],\n",
    "               'system_understanding': 'Manufacturing production line with varied stoppage reasons',\n",
    "               'preferred_interpretability': 'High - need clear cluster explanations'\n",
    "           }\n",
    "       \n",
    "       self.domain_expert_input = requirements\n",
    "       \n",
    "       # Method selection based on domain expert input (following research logic)\n",
    "       self.selected_method = 'hierarchical_clustering_with_density_comparison'\n",
    "       self.method_justification = {\n",
    "           'primary_method': 'Agglomerative Hierarchical Clustering (AHC)',\n",
    "           'comparison_method': 'HDBSCAN',\n",
    "           'selection_rationale': [\n",
    "               'AHC chosen following research methodology for hierarchical structure',\n",
    "               'Complete linkage provides clear separation between stoppage patterns',\n",
    "               'HDBSCAN added for density-based analysis and outlier detection',\n",
    "               'Both methods suitable for event log time series from manufacturing'\n",
    "           ],\n",
    "           'domain_expert_requirements_met': [\n",
    "               'Hierarchical structure aids understanding of stoppage relationships',\n",
    "               'Density-based clustering identifies anomalous patterns',\n",
    "               'Both methods provide interpretable results for manufacturing context',\n",
    "               'Comparison allows selection of optimal approach'\n",
    "           ],\n",
    "           'research_alignment': 'Following paper methodology with extended comparison study'\n",
    "       }\n",
    "       \n",
    "       print(\"✓ Domain expert input collected\")\n",
    "       print(f\"  Target analysis: {requirements['target_analysis']}\")\n",
    "       print(f\"  Method selected: {self.selected_method}\")\n",
    "   \n",
    "   def _verify_data_suitability(self, event_data):\n",
    "       \"\"\"\n",
    "       Verify data suitability for selected method (feedback loop with Module 1)\n",
    "       \"\"\"\n",
    "       print(\"--- Data Suitability Verification (Feedback Loop) ---\")\n",
    "       \n",
    "       # Check if necessary information can be extracted (following research)\n",
    "       verification_checks = {}\n",
    "       \n",
    "       # Check 1: Time series data availability\n",
    "       has_timestamps = 'Start Datetime' in event_data.columns and 'End Datetime' in event_data.columns\n",
    "       verification_checks['time_series_data'] = has_timestamps\n",
    "       \n",
    "       # Check 2: Event classification data\n",
    "       has_event_types = 'Stoppage Reason' in event_data.columns\n",
    "       verification_checks['event_classification'] = has_event_types\n",
    "       \n",
    "       # Check 3: Sufficient data volume\n",
    "       sufficient_volume = len(event_data) >= 10  # Minimum for meaningful clustering\n",
    "       verification_checks['data_volume'] = sufficient_volume\n",
    "       \n",
    "       # Check 4: Time interval coverage\n",
    "       if has_timestamps:\n",
    "           time_span = (event_data['Start Datetime'].max() - event_data['Start Datetime'].min()).days\n",
    "           adequate_timespan = time_span >= 1\n",
    "           verification_checks['time_coverage'] = adequate_timespan\n",
    "       else:\n",
    "           verification_checks['time_coverage'] = False\n",
    "       \n",
    "       # Check 5: Event diversity\n",
    "       if has_event_types:\n",
    "           event_diversity = event_data['Stoppage Reason'].nunique() >= 2\n",
    "           verification_checks['event_diversity'] = event_diversity\n",
    "       else:\n",
    "           verification_checks['event_diversity'] = False\n",
    "       \n",
    "       self.data_verification = verification_checks\n",
    "       \n",
    "       # Verify overall suitability\n",
    "       all_checks_passed = all(verification_checks.values())\n",
    "       \n",
    "       print(\"✓ Data verification completed:\")\n",
    "       for check, result in verification_checks.items():\n",
    "           status = \"✓\" if result else \"✗\"\n",
    "           print(f\"  {status} {check}: {result}\")\n",
    "       \n",
    "       if not all_checks_passed:\n",
    "           print(\"⚠ Warning: Some data requirements not met. Consider data collection refinement.\")\n",
    "           # In practice, this would trigger feedback to Module 1\n",
    "       else:\n",
    "           print(\"✓ All data requirements met for selected method\")\n",
    "       \n",
    "       return all_checks_passed\n",
    "   \n",
    "   def _finalize_method_selection(self):\n",
    "       \"\"\"\n",
    "       Finalize method selection based on domain input and data verification\n",
    "       \"\"\"\n",
    "       print(\"--- Method Selection Finalization ---\")\n",
    "       \n",
    "       # Confirm final selection (following research methodology)\n",
    "       print(f\"✓ Final method selection: {self.selected_method}\")\n",
    "       print(\"✓ Method components:\")\n",
    "       print(f\"  - Primary: {self.method_justification['primary_method']}\")\n",
    "       print(f\"  - Comparison: {self.method_justification['comparison_method']}\")\n",
    "       print(\"✓ Rationale:\")\n",
    "       for rationale in self.method_justification['selection_rationale']:\n",
    "           print(f\"  - {rationale}\")\n",
    "       \n",
    "       # Set method parameters (following research specifications)\n",
    "       self.method_parameters = {\n",
    "           'ahc_linkage': 'complete',  # Following research methodology\n",
    "           'distance_metric': 'dtw',   # As per research paper\n",
    "           'hdbscan_min_cluster_size': 'adaptive',  # Based on data size\n",
    "           'clustering_features': [\n",
    "               'duration_patterns', 'temporal_patterns', 'reason_encoding'\n",
    "           ]\n",
    "       }\n",
    "       \n",
    "       print(\"✓ Method parameters configured following research specifications\")\n",
    "# =============================================================================\n",
    "# MODULE 3: DATA PRE-PROCESSING\n",
    "# Following research methodology for event log preprocessing\n",
    "# =============================================================================\n",
    "class Module3_DataPreprocessing:\n",
    "   \"\"\"\n",
    "   Module 3: Data pre-processing following research methodology\n",
    "   Preparing event log data for clustering analysis\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.processed_data = None\n",
    "       self.time_series_data = None\n",
    "       self.preprocessing_steps = []\n",
    "       \n",
    "   def preprocess_event_data(self, raw_data):\n",
    "    \"\"\"\n",
    "    Preprocess event log data following research methodology\n",
    "    Computing metrics for each machine/line in each production run\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MODULE 3: DATA PRE-PROCESSING ===\")\n",
    "    print(\"Following research methodology for event log preprocessing\")\n",
    "    print(\"Computing metrics for stoppage pattern analysis\")\n",
    "    \n",
    "    self.processed_data = raw_data.copy()\n",
    "    \n",
    "    # Step 1: Data cleaning (research methodology)\n",
    "    self._clean_event_data()\n",
    "    \n",
    "    # Step 2: Event classification (domain expert definitions) - FIX THE METHOD CALL\n",
    "    event_definitions = self._classify_events()  # This method needs to exist\n",
    "    \n",
    "    # Step 3: Metric computation (adapted from research)\n",
    "    self._compute_stoppage_metrics()\n",
    "    \n",
    "    # Step 4: Time series generation (following research approach)\n",
    "    self._generate_time_series_features()\n",
    "    \n",
    "    # Step 5: Feature engineering for clustering\n",
    "    self._engineer_clustering_features()\n",
    "    \n",
    "    print(f\"✓ Preprocessing complete following research methodology\")\n",
    "    print(f\"  Features for clustering: {len(self.clustering_features)}\")\n",
    "    print(f\"  Time series data generated for {self.processed_data['Line'].nunique()} lines\")\n",
    "    \n",
    "    return self.processed_data\n",
    "   \n",
    "   def _compute_stoppage_metrics(self):\n",
    "       \"\"\"\n",
    "       Compute stoppage metrics following research approach\n",
    "       Adapted from active period computation in original methodology\n",
    "       \"\"\"\n",
    "       print(\"--- Stoppage Metrics Computation (Research Approach) ---\")\n",
    "       \n",
    "       # Following research: compute metrics for each machine in each production run\n",
    "       # Adapted for stoppage analysis instead of active period\n",
    "       \n",
    "       # Compute core stoppage metrics (following research metric computation)\n",
    "       self.processed_data['Stoppage_Duration_Minutes'] = (\n",
    "           self.processed_data['End Datetime'] - self.processed_data['Start Datetime']\n",
    "       ).dt.total_seconds() / 60\n",
    "       \n",
    "       # Compute additional metrics for clustering analysis\n",
    "       # Following research approach of extracting relevant metrics\n",
    "       \n",
    "       # 1. Frequency metrics per line per shift\n",
    "       line_shift_stats = self.processed_data.groupby(['Line', 'Shift Id']).agg({\n",
    "           'Stoppage_Duration_Minutes': ['count', 'sum', 'mean', 'std'],\n",
    "           'Event_Category': lambda x: x.value_counts().index[0]  # Most common category\n",
    "       }).round(2)\n",
    "       \n",
    "       # 2. Time-based metrics\n",
    "       self.processed_data['Time_Since_Shift_Start'] = self.processed_data.apply(\n",
    "           self._calculate_time_since_shift_start, axis=1\n",
    "       )\n",
    "       \n",
    "       # 3. Sequential patterns (inter-arrival analysis)\n",
    "       self.processed_data = self.processed_data.sort_values(['Line', 'Start Datetime'])\n",
    "       self.processed_data['Time_Since_Last_Stoppage'] = self.processed_data.groupby('Line')['Start Datetime'].diff().dt.total_seconds() / 60\n",
    "       self.processed_data['Time_Since_Last_Stoppage'].fillna(0, inplace=True)\n",
    "       \n",
    "       # 4. Impact scoring (following research metric approach)\n",
    "       self.processed_data['Impact_Score'] = self._calculate_impact_score()\n",
    "       \n",
    "       print(\"✓ Stoppage metrics computed following research methodology:\")\n",
    "       print(f\"  Duration metrics: mean={self.processed_data['Stoppage_Duration_Minutes'].mean():.1f} min\")\n",
    "       print(f\"  Frequency per line-shift: {line_shift_stats.shape[0]} combinations\")\n",
    "       print(f\"  Sequential patterns: inter-arrival times calculated\")\n",
    "       print(f\"  Impact scoring: range {self.processed_data['Impact_Score'].min():.2f}-{self.processed_data['Impact_Score'].max():.2f}\")\n",
    "       \n",
    "       self.preprocessing_steps.append('stoppage_metrics_computation')\n",
    "   \n",
    "   def _calculate_time_since_shift_start(self, row):\n",
    "       \"\"\"Calculate time since shift start (following research temporal analysis)\"\"\"\n",
    "       shift_start_hours = {'A': 0, 'B': 8, 'C': 16}  # Typical 8-hour shifts\n",
    "       shift_start = shift_start_hours.get(row['Shift Id'], 0)\n",
    "       current_hour = row['Start Datetime'].hour\n",
    "       \n",
    "       if current_hour >= shift_start:\n",
    "           return current_hour - shift_start\n",
    "       else:\n",
    "           return (24 - shift_start) + current_hour  # Handle overnight shifts\n",
    "   \n",
    "   def _calculate_impact_score(self):\n",
    "       \"\"\"\n",
    "       Calculate impact score following research approach\n",
    "       Combines duration, frequency, and operational impact\n",
    "       \"\"\"\n",
    "       # Normalize duration (0-1 scale)\n",
    "       duration_norm = (self.processed_data['Stoppage_Duration_Minutes'] - self.processed_data['Stoppage_Duration_Minutes'].min()) / (\n",
    "           self.processed_data['Stoppage_Duration_Minutes'].max() - self.processed_data['Stoppage_Duration_Minutes'].min()\n",
    "       )\n",
    "       \n",
    "       # Impact weights based on operational impact classification\n",
    "       impact_weights = {'Low': 0.3, 'Medium': 0.6, 'High': 1.0}\n",
    "       impact_multiplier = self.processed_data['Operational_Impact'].map(impact_weights)\n",
    "       \n",
    "       # Combined impact score\n",
    "       return duration_norm * impact_multiplier\n",
    "   \n",
    "   def _clean_event_data(self):\n",
    "       \"\"\"\n",
    "       Clean event log data following research methodology\n",
    "       Includes domain expert input for time interval definition\n",
    "       \"\"\"\n",
    "       print(\"--- Data Cleaning (Following Research Methodology) ---\")\n",
    "       initial_rows = len(self.processed_data)\n",
    "       \n",
    "       # Step 1: Remove events outside defined time intervals (research methodology)\n",
    "       # Domain experts define production run time intervals\n",
    "       production_run_hours = (6, 23)  # 06:00 to 23:00 as per research\n",
    "       self.processed_data['hour'] = self.processed_data['Start Datetime'].dt.hour\n",
    "       \n",
    "       # Filter to production hours (domain expert definition)\n",
    "       production_mask = (\n",
    "           (self.processed_data['hour'] >= production_run_hours[0]) & \n",
    "           (self.processed_data['hour'] <= production_run_hours[1])\n",
    "       )\n",
    "       self.processed_data = self.processed_data[production_mask]\n",
    "       \n",
    "       # Step 2: Remove duplicate events (research methodology)\n",
    "       duplicates_before = len(self.processed_data)\n",
    "       self.processed_data = self.processed_data.drop_duplicates(\n",
    "           subset=['Start Datetime', 'End Datetime', 'Stoppage Reason', 'Line']\n",
    "       )\n",
    "       duplicates_removed = duplicates_before - len(self.processed_data)\n",
    "       \n",
    "       # Step 3: Remove invalid timestamps\n",
    "       self.processed_data = self.processed_data.dropna(subset=['Start Datetime', 'End Datetime'])\n",
    "       \n",
    "       # Step 4: Remove negative durations\n",
    "       duration_check = (self.processed_data['End Datetime'] - self.processed_data['Start Datetime']).dt.total_seconds()\n",
    "       self.processed_data = self.processed_data[duration_check > 0]\n",
    "       \n",
    "       # Step 5: Remove events not of interest (domain expert input)\n",
    "       # In practice, domain experts would define which events to exclude\n",
    "       excluded_events = []  # Could include 'trial runs', 'calibration', etc.\n",
    "       if excluded_events:\n",
    "           self.processed_data = self.processed_data[\n",
    "               ~self.processed_data['Stoppage Reason'].isin(excluded_events)\n",
    "           ]\n",
    "       \n",
    "       cleaned_rows = len(self.processed_data)\n",
    "       print(f\"✓ Data cleaning completed (research methodology):\")\n",
    "       print(f\"  Initial events: {initial_rows}\")\n",
    "       print(f\"  Production hours filter: {production_mask.sum()} events retained\")\n",
    "       print(f\"  Duplicates removed: {duplicates_removed}\")\n",
    "       print(f\"  Final events: {cleaned_rows}\")\n",
    "       print(f\"  Total removed: {initial_rows - cleaned_rows} events\")\n",
    "       \n",
    "       self.preprocessing_steps.append('data_cleaning_research_methodology')\n",
    "   \n",
    "   def _generate_time_series_features(self):\n",
    "       \"\"\"\n",
    "       Generate time series features following research methodology\n",
    "       Creates matrix T(n x m) adapted for stoppage reasons instead of machines\n",
    "       \"\"\"\n",
    "       print(\"--- Time Series Generation (Research Methodology - Stoppage Reason Focus) ---\")\n",
    "       \n",
    "       # ADAPTATION: Focus on stoppage reasons instead of machines\n",
    "       # Following research: Let N = {1,2,3,…, n} be the set of n production runs\n",
    "       # and S representing the set of s stoppage reasons (instead of m machines)\n",
    "       \n",
    "       # Step 1: Define production runs and stoppage reasons\n",
    "       self.processed_data['Production_Date'] = self.processed_data['Start Datetime'].dt.date\n",
    "       production_runs = sorted(self.processed_data['Production_Date'].unique())\n",
    "       stoppage_reasons = sorted(self.processed_data['Stoppage Reason'].unique())\n",
    "       \n",
    "       n_runs = len(production_runs)\n",
    "       s_reasons = len(stoppage_reasons)\n",
    "       \n",
    "       print(f\"✓ Production runs identified: {n_runs} runs\")\n",
    "       print(f\"✓ Stoppage reasons identified: {s_reasons} reasons\")\n",
    "       print(f\"✓ Creating matrix T({n_runs} x {s_reasons}) - STOPPAGE REASON FOCUS\")\n",
    "       \n",
    "       # Step 2: Compute metric for each stoppage reason across each production run\n",
    "       # Following research: compute active durations adapted to stoppage reason patterns\n",
    "       \n",
    "       # Create the time series matrix T(n x s) following research format\n",
    "       # Rows = production runs, Columns = stoppage reasons\n",
    "       self.time_series_matrix = pd.DataFrame(\n",
    "           index=production_runs,\n",
    "           columns=stoppage_reasons,\n",
    "           dtype=float\n",
    "       )\n",
    "       \n",
    "       # Compute metrics for each cell t_ij (production run i, stoppage reason j)\n",
    "       for run_date in production_runs:\n",
    "           for reason in stoppage_reasons:\n",
    "               # Filter data for this production run and stoppage reason\n",
    "               run_reason_data = self.processed_data[\n",
    "                   (self.processed_data['Production_Date'] == run_date) &\n",
    "                   (self.processed_data['Stoppage Reason'] == reason)\n",
    "               ]\n",
    "               \n",
    "               if len(run_reason_data) > 0:\n",
    "                   # Compute aggregate stoppage duration for this run-reason combination\n",
    "                   # Following research approach of aggregating durations per production run\n",
    "                   total_stoppage_duration = run_reason_data['Stoppage_Duration_Minutes'].sum()\n",
    "                   \n",
    "                   # Store in matrix (following research T matrix format)\n",
    "                   self.time_series_matrix.loc[run_date, reason] = total_stoppage_duration\n",
    "               else:\n",
    "                   # No stoppages recorded for this run-reason combination\n",
    "                   self.time_series_matrix.loc[run_date, reason] = 0.0\n",
    "       \n",
    "       # Step 3: Normalize to uniform scale (following research methodology)\n",
    "       # Research: \"express as percentage of scheduled hours of production run\"\n",
    "       # Adapted: normalize stoppage durations to handle scale differences\n",
    "       \n",
    "       # Assume 17-hour production runs (06:00 to 23:00 as per research)\n",
    "       scheduled_hours_per_run = 17\n",
    "       scheduled_minutes_per_run = scheduled_hours_per_run * 60\n",
    "       \n",
    "       # Convert to percentage of scheduled time (following research normalization)\n",
    "       self.time_series_matrix_normalized = (self.time_series_matrix / scheduled_minutes_per_run) * 100\n",
    "       \n",
    "       # Step 4: Handle missing values and create time series features\n",
    "       self.time_series_matrix_normalized = self.time_series_matrix_normalized.fillna(0)\n",
    "       \n",
    "       # Step 5: Generate individual time series for each stoppage reason\n",
    "       self.stoppage_reason_time_series = {}\n",
    "       for reason in stoppage_reasons:\n",
    "           self.stoppage_reason_time_series[reason] = self.time_series_matrix_normalized[reason].values\n",
    "       \n",
    "       print(\"✓ Time series matrix T(n x s) generated following research methodology:\")\n",
    "       print(f\"  Matrix shape: {self.time_series_matrix.shape}\")\n",
    "       print(f\"  Focus: Stoppage reasons instead of machines\")\n",
    "       print(f\"  Normalization: percentage of scheduled production time\")\n",
    "       print(f\"  Value range: {self.time_series_matrix_normalized.min().min():.2f}% - {self.time_series_matrix_normalized.max().max():.2f}%\")\n",
    "       print(f\"  Sample values per stoppage reason:\")\n",
    "       for reason in stoppage_reasons[:3]:  # Show first 3 stoppage reasons\n",
    "           avg_stoppage = self.time_series_matrix_normalized[reason].mean()\n",
    "           print(f\"    '{reason}': avg {avg_stoppage:.2f}% of production time\")\n",
    "       \n",
    "       # Step 6: Add temporal features to processed data (maintaining existing approach)\n",
    "       self.processed_data['Hour_of_Day'] = self.processed_data['Start Datetime'].dt.hour\n",
    "       self.processed_data['Day_of_Week'] = self.processed_data['Start Datetime'].dt.dayofweek\n",
    "       self.processed_data['Month'] = self.processed_data['Start Datetime'].dt.month\n",
    "       self.processed_data['Day_of_Month'] = self.processed_data['Start Datetime'].dt.day\n",
    "       \n",
    "       # Shift encoding (production context)\n",
    "       shift_encoder = LabelEncoder()\n",
    "       self.processed_data['Shift_Encoded'] = shift_encoder.fit_transform(self.processed_data['Shift Id'])\n",
    "       \n",
    "       # Inter-arrival times (time series spacing analysis)\n",
    "       self.processed_data = self.processed_data.sort_values('Start Datetime')\n",
    "       self.processed_data['Inter_Arrival_Minutes'] = (\n",
    "           self.processed_data['Start Datetime'].diff().dt.total_seconds() / 60\n",
    "       ).fillna(0)\n",
    "       \n",
    "       print(\"✓ Additional temporal features generated\")\n",
    "       self.preprocessing_steps.append('time_series_generation_stoppage_reason_focus')\n",
    "       \n",
    "       return self.time_series_matrix_normalized\n",
    "   \n",
    "   def plot_time_series_like_research(self):\n",
    "       \"\"\"\n",
    "       Create time series plots similar to Figure 3 in research paper\n",
    "       Shows temporal patterns for each stoppage reason (not machines)\n",
    "       \"\"\"\n",
    "       print(\"--- Time Series Visualization (Research Figure 3 Style - Stoppage Reasons) ---\")\n",
    "       \n",
    "       n_reasons = len(self.stoppage_reason_time_series)\n",
    "       fig, axes = plt.subplots(n_reasons, 1, figsize=(12, 3*n_reasons))\n",
    "       if n_reasons == 1:\n",
    "           axes = [axes]\n",
    "       \n",
    "       production_runs = range(len(list(self.stoppage_reason_time_series.values())[0]))\n",
    "       \n",
    "       for idx, (reason, time_series) in enumerate(self.stoppage_reason_time_series.items()):\n",
    "           axes[idx].plot(production_runs, time_series, 'b-', linewidth=1.5, alpha=0.7)\n",
    "           axes[idx].fill_between(production_runs, time_series, alpha=0.3)\n",
    "           axes[idx].set_title(f\"'{reason}' - Stoppage Time Series\")\n",
    "           axes[idx].set_xlabel('Production Run')\n",
    "           axes[idx].set_ylabel('Stoppage Time (%)')\n",
    "           axes[idx].grid(True, alpha=0.3)\n",
    "           \n",
    "           # Add trend line\n",
    "           z = np.polyfit(production_runs, time_series, 1)\n",
    "           p = np.poly1d(z)\n",
    "           axes[idx].plot(production_runs, p(production_runs), \"r--\", alpha=0.8, linewidth=2)\n",
    "       \n",
    "       plt.suptitle('Stoppage Reason Time Series - Pattern Analysis\\n(Following Research Paper Figure 3 Style)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "       \n",
    "       print(\"✓ Time series plots generated (research paper style - stoppage reason focus)\")\n",
    "       print(\"  Note: Similar to research Figure 3, patterns are complex and require ML analysis\")\n",
    "   \n",
    "   # In the Module3_DataPreprocessing class, update the _engineer_clustering_features method:\n",
    "\n",
    "def _engineer_clustering_features(self):\n",
    "    \"\"\"Engineer features specifically for clustering analysis\"\"\"\n",
    "    print(\"--- Clustering Feature Engineering ---\")\n",
    "    \n",
    "    # Stoppage reason encoding\n",
    "    reason_encoder = LabelEncoder()\n",
    "    self.processed_data['Reason_Encoded'] = reason_encoder.fit_transform(self.processed_data['Stoppage Reason'])\n",
    "    \n",
    "    # Cyclic encoding for temporal features (handle cyclical nature)\n",
    "    self.processed_data['Hour_Sin'] = np.sin(2 * np.pi * self.processed_data['Hour_of_Day'] / 24)\n",
    "    self.processed_data['Hour_Cos'] = np.cos(2 * np.pi * self.processed_data['Hour_of_Day'] / 24)\n",
    "    self.processed_data['DayOfWeek_Sin'] = np.sin(2 * np.pi * self.processed_data['Day_of_Week'] / 7)\n",
    "    self.processed_data['DayOfWeek_Cos'] = np.cos(2 * np.pi * self.processed_data['Day_of_Week'] / 7)\n",
    "    \n",
    "    # Update clustering features to include new classifications - FIX THE COLUMN NAME\n",
    "    self.clustering_features = [\n",
    "        'Stoppage_Duration_Minutes', 'Inter_Arrival_Minutes', 'Hour_Sin', 'Hour_Cos',  # CHANGED Duration_Minutes to Stoppage_Duration_Minutes\n",
    "        'DayOfWeek_Sin', 'DayOfWeek_Cos', 'Shift_Encoded', 'Reason_Encoded',\n",
    "        'Event_Category_Encoded', 'Operational_Impact_Encoded'\n",
    "    ]\n",
    "    \n",
    "    print(f\"✓ Clustering features: {self.clustering_features}\")\n",
    "    self.preprocessing_steps.append('clustering_feature_engineering')\n",
    "   \n",
    "    def _classify_events(self):\n",
    "       \"\"\"\n",
    "       Classify events based on domain expert definitions (following research methodology)\n",
    "       \"\"\"\n",
    "       print(\"--- Event Classification (Domain Expert Definitions) ---\")\n",
    "       \n",
    "       # Following research methodology: domain experts provide event definitions\n",
    "       # Research used 7 distinct event types classified as active/inactive\n",
    "       # We adapt this for stoppage classification based on operational impact\n",
    "       \n",
    "       def get_domain_expert_definitions():\n",
    "           \"\"\"\n",
    "           Simulate domain expert event definitions (following research approach)\n",
    "           In practice, these would be provided by manufacturing domain experts\n",
    "           \"\"\"\n",
    "           return {\n",
    "               'CIL': {\n",
    "                   'classification': 'Production_Issue',\n",
    "                   'operational_impact': 'High',\n",
    "                   'definition': 'Critical production interruption requiring immediate attention'\n",
    "               },\n",
    "               'End of Shift Cleaning': {\n",
    "                   'classification': 'Planned_Maintenance',\n",
    "                   'operational_impact': 'Low',\n",
    "                   'definition': 'Scheduled cleaning at shift end'\n",
    "               },\n",
    "               'Intermittent Cleaning': {\n",
    "                   'classification': 'Planned_Maintenance', \n",
    "                   'operational_impact': 'Medium',\n",
    "                   'definition': 'Regular cleaning during production'\n",
    "               },\n",
    "               'Punch Tip Checks': {\n",
    "                   'classification': 'Quality_Control',\n",
    "                   'operational_impact': 'Medium',\n",
    "                   'definition': 'Quality assurance inspection'\n",
    "               },\n",
    "               'Machine Breakdown': {\n",
    "                   'classification': 'Equipment_Failure',\n",
    "                   'operational_impact': 'High',\n",
    "                   'definition': 'Unplanned equipment failure'\n",
    "               },\n",
    "               'Tool Change': {\n",
    "                   'classification': 'Setup_Changeover',\n",
    "                   'operational_impact': 'Medium',\n",
    "                   'definition': 'Tool replacement or setup change'\n",
    "               },\n",
    "               'Quality Check': {\n",
    "                   'classification': 'Quality_Control',\n",
    "                   'operational_impact': 'Medium',\n",
    "                   'definition': 'Product quality inspection'\n",
    "               },\n",
    "               'Material Shortage': {\n",
    "                   'classification': 'Material_Issue',\n",
    "                   'operational_impact': 'High',\n",
    "                   'definition': 'Lack of required materials'\n",
    "               },\n",
    "               'Operator Break': {\n",
    "                   'classification': 'Operator_Related',\n",
    "                   'operational_impact': 'Low',\n",
    "                   'definition': 'Scheduled operator break'\n",
    "               },\n",
    "               'Machine Setup': {\n",
    "                   'classification': 'Setup_Changeover',\n",
    "                   'operational_impact': 'Medium',\n",
    "                   'definition': 'Machine configuration for production'\n",
    "               },\n",
    "               'Preventive Maintenance': {\n",
    "                   'classification': 'Planned_Maintenance',\n",
    "                   'operational_impact': 'Low',\n",
    "                   'definition': 'Scheduled preventive maintenance'\n",
    "               }\n",
    "           }\n",
    "       \n",
    "       # Get domain expert definitions (following research methodology)\n",
    "       event_definitions = get_domain_expert_definitions()\n",
    "       \n",
    "       # Apply classifications based on domain expert definitions\n",
    "       def classify_stoppage_reason(reason):\n",
    "           if reason in event_definitions:\n",
    "               return event_definitions[reason]['classification']\n",
    "           else:\n",
    "               # Handle unknown events (would require domain expert input in practice)\n",
    "               reason_lower = reason.lower()\n",
    "               if 'cleaning' in reason_lower or 'maintenance' in reason_lower:\n",
    "                   return 'Planned_Maintenance'\n",
    "               elif 'check' in reason_lower or 'quality' in reason_lower:\n",
    "                   return 'Quality_Control'\n",
    "               elif 'breakdown' in reason_lower or 'failure' in reason_lower:\n",
    "                   return 'Equipment_Failure'\n",
    "               elif 'setup' in reason_lower or 'change' in reason_lower:\n",
    "                   return 'Setup_Changeover'\n",
    "               elif 'operator' in reason_lower or 'break' in reason_lower:\n",
    "                   return 'Operator_Related'\n",
    "               elif 'material' in reason_lower:\n",
    "                   return 'Material_Issue'\n",
    "               else:\n",
    "                   return 'Other'\n",
    "       \n",
    "       def get_operational_impact(reason):\n",
    "           if reason in event_definitions:\n",
    "               return event_definitions[reason]['operational_impact']\n",
    "           else:\n",
    "               return 'Medium'  # Default for unknown events\n",
    "       \n",
    "       # Apply classifications (following research methodology)\n",
    "       self.processed_data['Event_Category'] = self.processed_data['Stoppage Reason'].apply(classify_stoppage_reason)\n",
    "       self.processed_data['Operational_Impact'] = self.processed_data['Stoppage Reason'].apply(get_operational_impact)\n",
    "       \n",
    "       # Create encoded versions for clustering\n",
    "       category_encoder = LabelEncoder()\n",
    "       impact_encoder = LabelEncoder()\n",
    "       \n",
    "       self.processed_data['Event_Category_Encoded'] = category_encoder.fit_transform(self.processed_data['Event_Category'])\n",
    "       self.processed_data['Operational_Impact_Encoded'] = impact_encoder.fit_transform(self.processed_data['Operational_Impact'])\n",
    "       \n",
    "       # Display classification results (following research reporting)\n",
    "       print(\"✓ Event classification completed (domain expert definitions):\")\n",
    "       \n",
    "       event_distribution = self.processed_data['Event_Category'].value_counts()\n",
    "       print(f\"  Event categories: {dict(event_distribution)}\")\n",
    "       \n",
    "       impact_distribution = self.processed_data['Operational_Impact'].value_counts()\n",
    "       print(f\"  Impact levels: {dict(impact_distribution)}\")\n",
    "       \n",
    "       # Show sample classifications\n",
    "       print(\"  Sample event definitions:\")\n",
    "       for reason in self.processed_data['Stoppage Reason'].unique()[:5]:\n",
    "           category = self.processed_data[self.processed_data['Stoppage Reason'] == reason]['Event_Category'].iloc[0]\n",
    "           impact = self.processed_data[self.processed_data['Stoppage Reason'] == reason]['Operational_Impact'].iloc[0]\n",
    "           print(f\"    '{reason}' → {category} ({impact} impact)\")\n",
    "       \n",
    "       self.preprocessing_steps.append('event_classification_domain_expert')\n",
    "       \n",
    "       # Store encoders for later use\n",
    "       self.encoders = {\n",
    "           'category_encoder': category_encoder,\n",
    "           'impact_encoder': impact_encoder\n",
    "       }\n",
    "       \n",
    "       return event_definitions\n",
    "# =============================================================================\n",
    "# MODULE 4: DTW DISTANCE AND HIERARCHICAL CLUSTERING\n",
    "# Following research methodology with HDBSCAN comparison\n",
    "# =============================================================================\n",
    "class Module4_DTWClustering:\n",
    "   \"\"\"\n",
    "   Module 4: DTW distance calculation and agglomerative hierarchical clustering\n",
    "   Following research methodology exactly as described in paper\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.distance_matrix = None\n",
    "       self.clustering_results = {}\n",
    "       self.dendrogram_data = None\n",
    "       \n",
    "   def apply_dtw_clustering(self, processed_data, time_series_matrix, clustering_features):\n",
    "       \"\"\"\n",
    "       Apply DTW-based hierarchical clustering following research methodology\n",
    "       FOCUS: Stoppage reasons instead of machines\n",
    "       \"\"\"\n",
    "       print(\"\\n=== MODULE 4: GENERATING A DENDROGRAM ===\")\n",
    "       print(\"Following research methodology for DTW distance and agglomerative clustering\")\n",
    "       print(\"ADAPTATION: Clustering stoppage reasons instead of machines\")\n",
    "       print(\"Implementing complete linkage hierarchical clustering as per research\")\n",
    "       \n",
    "       # Step 1: Prepare data for clustering (stoppage reason focus)\n",
    "       self._prepare_clustering_data_stoppage_focus(processed_data, time_series_matrix, clustering_features)\n",
    "       \n",
    "       # Step 2: Calculate DTW distance matrix (research methodology)\n",
    "       self._calculate_dtw_distances_research_method()\n",
    "       \n",
    "       # Step 3: Apply agglomerative hierarchical clustering (exact research approach)\n",
    "       self._apply_agglomerative_clustering_research()\n",
    "       \n",
    "       # Step 4: Apply HDBSCAN for comparison (our addition)\n",
    "       self._apply_hdbscan_comparison()\n",
    "       \n",
    "       # Step 5: Generate dendrogram (following research visualization)\n",
    "       self._generate_dendrogram_research_style()\n",
    "       \n",
    "       return self.clustering_results\n",
    "   \n",
    "   def _prepare_clustering_data_stoppage_focus(self, processed_data, time_series_matrix, clustering_features):\n",
    "       \"\"\"\n",
    "       Prepare data for clustering following research approach\n",
    "       FOCUS: Stoppage reasons as the clustering entities\n",
    "       \"\"\"\n",
    "       print(\"--- Data Preparation for Clustering (Stoppage Reason Focus) ---\")\n",
    "       \n",
    "       # Method 1: Time series matrix approach (following research T(n x s))\n",
    "       # where s = stoppage reasons instead of m = machines\n",
    "       self.time_series_data = time_series_matrix.T  # Transpose to get stoppage reasons as rows\n",
    "       print(f\"✓ Time series matrix prepared: {self.time_series_data.shape} (stoppage_reasons x production_runs)\")\n",
    "       print(f\"  Stoppage reasons to cluster: {list(self.time_series_data.index)}\")\n",
    "       \n",
    "       # Method 2: Feature-based approach (for HDBSCAN comparison)\n",
    "       X = processed_data[clustering_features].copy()\n",
    "       X = X.fillna(X.mean())\n",
    "       \n",
    "       # Standardize features (important for DTW)\n",
    "       scaler = StandardScaler()\n",
    "       self.X_scaled = scaler.fit_transform(X)\n",
    "       print(f\"✓ Feature matrix prepared: {self.X_scaled.shape}\")\n",
    "       \n",
    "       # Store both approaches for clustering comparison\n",
    "       self.clustering_data = {\n",
    "           'time_series': self.time_series_data.values,  # For DTW clustering (research method)\n",
    "           'features': self.X_scaled  # For HDBSCAN comparison\n",
    "       }\n",
    "       \n",
    "       print(f\"✓ Research adaptation: Clustering {len(self.time_series_data)} stoppage reasons\")\n",
    "       print(f\"  Original research clustered machines, we cluster stoppage reason patterns\")\n",
    "   \n",
    "   def _calculate_dtw_distances_research_method(self):\n",
    "       \"\"\"\n",
    "       Calculate DTW distance matrix following exact research methodology\n",
    "       ADAPTED: For stoppage reasons instead of machines\n",
    "       \"\"\"\n",
    "       print(\"--- DTW Distance Matrix Calculation (Research Method - Stoppage Reasons) ---\")\n",
    "       print(\"Implementing DTW as described in research for time-shifted pattern recognition\")\n",
    "       print(\"ADAPTATION: Computing distances between stoppage reason patterns\")\n",
    "       \n",
    "       # Use time series data (stoppage reasons as samples, production runs as time dimension)\n",
    "       time_series_data = self.clustering_data['time_series']\n",
    "       n_stoppage_reasons = time_series_data.shape[0]\n",
    "       \n",
    "       # Initialize distance matrix\n",
    "       self.distance_matrix = np.zeros((n_stoppage_reasons, n_stoppage_reasons))\n",
    "       \n",
    "       print(f\"Computing DTW distances for {n_stoppage_reasons} stoppage reasons...\")\n",
    "       print(\"Following research rationale: DTW removes time-shifts in stoppage patterns\")\n",
    "       \n",
    "       # Calculate pairwise DTW distances (following research methodology)\n",
    "       for i in range(n_stoppage_reasons):\n",
    "           for j in range(i+1, n_stoppage_reasons):\n",
    "               try:\n",
    "                   # DTW distance calculation (research approach)\n",
    "                   # Research notes: \"DTW can remove time-shifts by wrapping the time axis\"\n",
    "                   # Adapted: For stoppage reason temporal patterns\n",
    "                   ts1 = time_series_data[i].reshape(-1, 1)\n",
    "                   ts2 = time_series_data[j].reshape(-1, 1)\n",
    "                   \n",
    "                   distance, _ = fastdtw(ts1, ts2, dist=euclidean)\n",
    "                   self.distance_matrix[i, j] = distance\n",
    "                   self.distance_matrix[j, i] = distance\n",
    "                   \n",
    "               except Exception as e:\n",
    "                   # Fallback to Euclidean distance if DTW fails\n",
    "                   distance = euclidean(time_series_data[i], time_series_data[j])\n",
    "                   self.distance_matrix[i, j] = distance\n",
    "                   self.distance_matrix[j, i] = distance\n",
    "       \n",
    "       print(\"✓ DTW distance matrix calculated following research methodology:\")\n",
    "       print(f\"  Matrix shape: {self.distance_matrix.shape}\")\n",
    "       print(f\"  Distance range: {self.distance_matrix.min():.3f} - {self.distance_matrix.max():.3f}\")\n",
    "       print(\"  DTW captures time-shifted patterns in stoppage reason behavior (adapted from research)\")\n",
    "       \n",
    "       # Display stoppage reason names being clustered\n",
    "       stoppage_reasons = list(self.time_series_data.index)\n",
    "       print(f\"  Stoppage reasons being clustered: {stoppage_reasons}\")\n",
    "   \n",
    "   def _apply_agglomerative_clustering_research(self):\n",
    "       \"\"\"\n",
    "       Apply agglomerative hierarchical clustering exactly as described in research\n",
    "       ADAPTED: For stoppage reasons instead of machines\n",
    "       \"\"\"\n",
    "       print(\"--- Agglomerative Hierarchical Clustering (Exact Research Method - Stoppage Reasons) ---\")\n",
    "       print(\"Following research strategy: complete linkage agglomerative clustering\")\n",
    "       print(\"ADAPTATION: Clustering stoppage reasons instead of machines\")\n",
    "       \n",
    "       # Research methodology: \"agglomerative hierarchical clustering is suitable for bottleneck detection\"\n",
    "       # ADAPTED: \"generates a complete tree, starting with individual stoppage reasons\"\n",
    "       \n",
    "       # Step 1: Generate linkage matrix for dendrogram (research approach)\n",
    "       # Using complete linkage as specified in research\n",
    "       self.linkage_matrix = linkage(\n",
    "           squareform(self.distance_matrix),  # Convert to condensed distance matrix\n",
    "           method='complete'  # Following research specification\n",
    "       )\n",
    "       \n",
    "       # Step 2: Determine optimal number of clusters\n",
    "       optimal_clusters = self._find_optimal_clusters_research_method()\n",
    "       \n",
    "       # Step 3: Apply clustering with optimal number of clusters\n",
    "       ahc_clusterer = AgglomerativeClustering(\n",
    "           n_clusters=optimal_clusters,\n",
    "           linkage='complete',  # Following research methodology exactly\n",
    "           metric='precomputed'\n",
    "       )\n",
    "       \n",
    "       ahc_labels = ahc_clusterer.fit_predict(self.distance_matrix)\n",
    "       \n",
    "       # Store results following research format\n",
    "       stoppage_reason_names = list(self.time_series_data.index)\n",
    "       self.clustering_results['AHC_Research'] = {\n",
    "           'clusterer': ahc_clusterer,\n",
    "           'labels': ahc_labels,\n",
    "           'n_clusters': optimal_clusters,\n",
    "           'linkage_method': 'complete',  # Exact research specification\n",
    "           'distance_metric': 'DTW',      # Following research choice\n",
    "           'methodology': 'Research Paper Implementation',\n",
    "           'stoppage_reason_names': stoppage_reason_names,  # CHANGED from machine_names\n",
    "           'clustering_focus': 'stoppage_reasons'  # NEW: indicate our focus\n",
    "       }\n",
    "       \n",
    "       print(f\"✓ Agglomerative clustering completed (research methodology - stoppage reason focus):\")\n",
    "       print(f\"  Method: Complete linkage (following research)\")\n",
    "       print(f\"  Distance: DTW (research specification)\")\n",
    "       print(f\"  Clusters: {optimal_clusters}\")\n",
    "       print(f\"  Stoppage reason distribution: {np.bincount(ahc_labels)}\")\n",
    "       \n",
    "       # Display cluster assignments in research style (adapted)\n",
    "       print(\"  Stoppage reason cluster assignments:\")\n",
    "       for cluster_id in range(optimal_clusters):\n",
    "           reasons_in_cluster = [stoppage_reason_names[i] for i, label in enumerate(ahc_labels) if label == cluster_id]\n",
    "           print(f\"    Cluster {cluster_id}: {reasons_in_cluster}\")\n",
    "   \n",
    "   def _find_optimal_clusters_research_method(self):\n",
    "       \"\"\"\n",
    "       Find optimal number of clusters following research approach\n",
    "       Using silhouette analysis and dendrogram interpretation\n",
    "       \"\"\"\n",
    "       print(\"--- Optimal Cluster Number Determination (Research Method) ---\")\n",
    "       \n",
    "       # Method 1: Silhouette analysis (research validation approach)\n",
    "       max_clusters = min(8, len(self.distance_matrix) - 1)\n",
    "       silhouette_scores = []\n",
    "       \n",
    "       for n_clusters in range(2, max_clusters + 1):\n",
    "           labels = fcluster(self.linkage_matrix, n_clusters, criterion='maxclust')\n",
    "           \n",
    "           # Adjust labels to start from 0 (for compatibility)\n",
    "           labels = labels - 1\n",
    "           \n",
    "           # Calculate silhouette score\n",
    "           if len(set(labels)) > 1:\n",
    "               score = silhouette_score(self.distance_matrix, labels, metric='precomputed')\n",
    "               silhouette_scores.append(score)\n",
    "           else:\n",
    "               silhouette_scores.append(-1)\n",
    "       \n",
    "       # Method 2: Dendrogram-based decision (research approach)\n",
    "       # Look for significant jumps in linkage distances\n",
    "       linkage_distances = self.linkage_matrix[:, 2]\n",
    "       distance_jumps = np.diff(linkage_distances)\n",
    "       \n",
    "       # Find elbow point in distance jumps\n",
    "       if len(distance_jumps) > 2:\n",
    "           elbow_point = np.argmax(distance_jumps) + 2  # +2 because we start from 2 clusters\n",
    "       else:\n",
    "           elbow_point = 2\n",
    "       \n",
    "       # Combine both methods (research best practice)\n",
    "       silhouette_optimal = np.argmax(silhouette_scores) + 2\n",
    "       \n",
    "       # Choose the method that gives more interpretable results\n",
    "       if abs(silhouette_optimal - elbow_point) <= 1:\n",
    "           optimal_clusters = silhouette_optimal\n",
    "       else:\n",
    "           # Use silhouette if significantly different, otherwise use elbow\n",
    "           optimal_clusters = silhouette_optimal if max(silhouette_scores) > 0.3 else elbow_point\n",
    "       \n",
    "       optimal_clusters = max(2, min(optimal_clusters, max_clusters))\n",
    "       \n",
    "       print(f\"✓ Optimal clusters determined following research methodology:\")\n",
    "       print(f\"  Silhouette method: {silhouette_optimal} clusters (score: {max(silhouette_scores):.3f})\")\n",
    "       print(f\"  Dendrogram elbow: {elbow_point} clusters\")\n",
    "       print(f\"  Final selection: {optimal_clusters} clusters\")\n",
    "       \n",
    "       return optimal_clusters\n",
    "   \n",
    "   def _apply_hdbscan_comparison(self):\n",
    "       \"\"\"\n",
    "       Apply HDBSCAN clustering for comparison with research method\n",
    "       \"\"\"\n",
    "       print(\"--- HDBSCAN Clustering (Comparison Method) ---\")\n",
    "       \n",
    "       # Use feature-based data for HDBSCAN (better suited than time series)\n",
    "       X_features = self.clustering_data['features']\n",
    "       \n",
    "       hdbscan_clusterer = hdbscan.HDBSCAN(\n",
    "           min_cluster_size=max(2, len(X_features) // 10),\n",
    "           min_samples=3,\n",
    "           metric='euclidean'\n",
    "       )\n",
    "       \n",
    "       hdbscan_labels = hdbscan_clusterer.fit_predict(X_features)\n",
    "       \n",
    "       n_clusters = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)\n",
    "       n_noise = list(hdbscan_labels).count(-1)\n",
    "       \n",
    "       self.clustering_results['HDBSCAN_Comparison'] = {\n",
    "           'clusterer': hdbscan_clusterer,\n",
    "           'labels': hdbscan_labels,\n",
    "           'n_clusters': n_clusters,\n",
    "           'n_noise': n_noise,\n",
    "           'distance_metric': 'Euclidean',\n",
    "           'methodology': 'Density-based comparison method'\n",
    "       }\n",
    "       \n",
    "       print(f\"✓ HDBSCAN comparison completed:\")\n",
    "       print(f\"  Clusters: {n_clusters}\")\n",
    "       print(f\"  Noise points: {n_noise}\")\n",
    "       if n_clusters > 0:\n",
    "           print(f\"  Distribution: {np.bincount(hdbscan_labels[hdbscan_labels >= 0])}\")\n",
    "   \n",
    "   def _generate_dendrogram_research_style(self):\n",
    "       \"\"\"\n",
    "       Generate dendrogram following research paper visualization style\n",
    "       ADAPTED: Shows stoppage reason relationships instead of machine relationships\n",
    "       \"\"\"\n",
    "       print(\"--- Dendrogram Generation (Research Paper Style - Stoppage Reasons) ---\")\n",
    "       \n",
    "       plt.figure(figsize=(12, 8))\n",
    "       \n",
    "       # Create dendrogram (following research visualization)\n",
    "       stoppage_reason_names = list(self.time_series_data.index)\n",
    "       \n",
    "       self.dendrogram_data = dendrogram(\n",
    "           self.linkage_matrix,\n",
    "           labels=stoppage_reason_names,\n",
    "           orientation='top',\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True\n",
    "       )\n",
    "       \n",
    "       plt.title('Agglomerative Hierarchical Clustering Dendrogram\\nStoppage Reason Pattern Clustering\\n(Following Research Paper Methodology)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "       plt.xlabel('Stoppage Reasons', fontsize=12)\n",
    "       plt.ylabel('DTW Distance', fontsize=12)\n",
    "       plt.xticks(rotation=45, ha='right')\n",
    "       \n",
    "       # Add horizontal line at optimal cut height\n",
    "       optimal_clusters = self.clustering_results['AHC_Research']['n_clusters']\n",
    "       if optimal_clusters > 1:\n",
    "           # Calculate cut height for optimal clusters\n",
    "           cut_height = self.linkage_matrix[-(optimal_clusters-1), 2]\n",
    "           plt.axhline(y=cut_height, color='r', linestyle='--', alpha=0.7, \n",
    "                      label=f'Cut for {optimal_clusters} clusters')\n",
    "           plt.legend()\n",
    "       \n",
    "       # Add research adaptation note\n",
    "       plt.text(0.02, 0.98, 'RESEARCH ADAPTATION:\\nClustering stoppage reasons\\ninstead of machines', \n",
    "               transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "       \n",
    "       print(\"✓ Dendrogram generated following research paper style\")\n",
    "       print(f\"  Shows hierarchical relationships between {len(stoppage_reason_names)} stoppage reasons\")\n",
    "       print(\"  Red dashed line indicates optimal cluster cut (research methodology)\")\n",
    "       print(\"  ADAPTATION: Stoppage reason clustering instead of machine clustering\")\n",
    "       \n",
    "       return self.dendrogram_data\n",
    "# =============================================================================\n",
    "# MODULE 5: CLUSTER COMPUTATION AND GENERATION\n",
    "# Following research methodology for cluster analysis\n",
    "# =============================================================================\n",
    "class Module5_ClusterGeneration:\n",
    "   \"\"\"\n",
    "   Module 5: Cluster computation and generation\n",
    "   Following research methodology for cluster analysis and validation\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.cluster_characteristics = {}\n",
    "       self.cluster_validation = {}\n",
    "       \n",
    "   def generate_clusters(self, processed_data, clustering_results, linkage_matrix, time_series_data):\n",
    "       \"\"\"\n",
    "       Generate and analyze clusters following research methodology\n",
    "       \"\"\"\n",
    "       print(\"\\n=== MODULE 5: CLUSTER COMPUTATION AND GENERATION ===\")\n",
    "       print(\"Following research methodology for cluster analysis\")\n",
    "       \n",
    "       # Add cluster labels to processed data\n",
    "       processed_data_with_clusters = processed_data.copy()\n",
    "       \n",
    "       # Map cluster assignments to the event data\n",
    "       if 'AHC_Research' in clustering_results:\n",
    "           # For stoppage reason clustering, need to map reason-based clusters to events\n",
    "           reason_cluster_map = {}\n",
    "           labels = clustering_results['AHC_Research']['labels']\n",
    "           stoppage_reasons = clustering_results['AHC_Research']['stoppage_reason_names']\n",
    "           \n",
    "           for i, reason in enumerate(stoppage_reasons):\n",
    "               reason_cluster_map[reason] = labels[i]\n",
    "           \n",
    "           processed_data_with_clusters['AHC_Cluster'] = processed_data_with_clusters['Stoppage Reason'].map(reason_cluster_map)\n",
    "       \n",
    "       if 'HDBSCAN_Comparison' in clustering_results:\n",
    "           # For HDBSCAN, assign clusters based on event features\n",
    "           hdbscan_labels = clustering_results['HDBSCAN_Comparison']['labels']\n",
    "           processed_data_with_clusters['HDBSCAN_Cluster'] = hdbscan_labels\n",
    "       \n",
    "       # Step 1: Compute cluster characteristics\n",
    "       self._compute_cluster_characteristics_research(processed_data_with_clusters, time_series_data)\n",
    "       \n",
    "       # Step 2: Validate cluster quality\n",
    "       self._validate_clusters_research(processed_data, clustering_results)\n",
    "       \n",
    "       # Step 3: Generate cluster profiles\n",
    "       self._generate_cluster_profiles(processed_data_with_clusters)\n",
    "       \n",
    "       # Store optimal clusters found\n",
    "       self.optimal_clusters = clustering_results.get('AHC_Research', {}).get('n_clusters', 2)\n",
    "       \n",
    "       return processed_data_with_clusters\n",
    "   \n",
    "   def _compute_cluster_characteristics_research(self, data_with_clusters, time_series_data):\n",
    "       \"\"\"\n",
    "       Compute cluster characteristics following research methodology\n",
    "       ADAPTED: Focus on stoppage reason groupings instead of machine groupings\n",
    "       \"\"\"\n",
    "       print(\"--- Cluster Characteristics Computation (Research Method - Stoppage Reason Focus) ---\")\n",
    "       \n",
    "       # Research approach: analyze clusters at stoppage reason level first, then event level\n",
    "       \n",
    "       # Method 1: Stoppage reason level cluster analysis (adapted from research machine focus)\n",
    "       if 'AHC_Cluster' in data_with_clusters.columns:\n",
    "           self._analyze_stoppage_reason_clusters_research(data_with_clusters, time_series_data)\n",
    "       \n",
    "       # Method 2: Event-level cluster analysis (our enhancement)\n",
    "       self._analyze_event_clusters_research(data_with_clusters)\n",
    "       \n",
    "       print(\"✓ Cluster characteristics computed following research methodology\")\n",
    "       print(\"✓ ADAPTATION: Focus on stoppage reason patterns instead of machine patterns\")\n",
    "   \n",
    "   def _analyze_stoppage_reason_clusters_research(self, data_with_clusters, time_series_data):\n",
    "       \"\"\"\n",
    "       Analyze stoppage reason clusters following research approach\n",
    "       Focus on time series patterns and stoppage reason groupings\n",
    "       \"\"\"\n",
    "       print(\"--- Stoppage Reason-Level Cluster Analysis (Research Focus - Adapted) ---\")\n",
    "       \n",
    "       self.cluster_characteristics['Stoppage_Reason_Level'] = {}\n",
    "       \n",
    "       # Get unique stoppage reason clusters\n",
    "       reason_clusters = data_with_clusters.groupby('Stoppage Reason')['AHC_Cluster'].first()\n",
    "       \n",
    "       for cluster_id in sorted(reason_clusters.unique()):\n",
    "           if pd.isna(cluster_id):\n",
    "               continue\n",
    "               \n",
    "           reasons_in_cluster = reason_clusters[reason_clusters == cluster_id].index.tolist()\n",
    "           \n",
    "           # Compute time series characteristics for this cluster\n",
    "           cluster_time_series = []\n",
    "           for reason in reasons_in_cluster:\n",
    "               if reason in time_series_data.index:\n",
    "                   cluster_time_series.append(time_series_data.loc[reason].values)\n",
    "           \n",
    "           if cluster_time_series:\n",
    "               cluster_time_series = np.array(cluster_time_series)\n",
    "               \n",
    "               # Compute cluster statistics (research approach)\n",
    "               cluster_stats = {\n",
    "                   'stoppage_reasons': reasons_in_cluster,  # CHANGED from 'machines'\n",
    "                   'n_reasons': len(reasons_in_cluster),    # CHANGED from 'n_machines'\n",
    "                   'time_series_stats': {\n",
    "                       'mean_pattern': np.mean(cluster_time_series, axis=0),\n",
    "                       'std_pattern': np.std(cluster_time_series, axis=0),\n",
    "                       'cluster_variance': np.var(cluster_time_series),\n",
    "                       'pattern_correlation': self._calculate_pattern_correlation(cluster_time_series)\n",
    "                   },\n",
    "                   'operational_characteristics': self._get_operational_characteristics(\n",
    "                       data_with_clusters, reasons_in_cluster, 'Stoppage Reason'\n",
    "                   )\n",
    "               }\n",
    "               \n",
    "               self.cluster_characteristics['Stoppage_Reason_Level'][f'Cluster_{cluster_id}'] = cluster_stats\n",
    "               \n",
    "               print(f\"  Cluster {cluster_id}: {reasons_in_cluster}\")\n",
    "               print(f\"    Stoppage reasons: {len(reasons_in_cluster)}\")  # CHANGED from 'Machines'\n",
    "               print(f\"    Avg impact rate: {np.mean(cluster_stats['time_series_stats']['mean_pattern']):.2f}%\")  # CHANGED\n",
    "               print(f\"    Pattern correlation: {cluster_stats['time_series_stats']['pattern_correlation']:.3f}\")\n",
    "   \n",
    "   def _calculate_pattern_correlation(self, time_series_array):\n",
    "       \"\"\"\n",
    "       Calculate pattern correlation within cluster (research metric)\n",
    "       \"\"\"\n",
    "       if len(time_series_array) < 2:\n",
    "           return 1.0\n",
    "       \n",
    "       correlations = []\n",
    "       for i in range(len(time_series_array)):\n",
    "           for j in range(i+1, len(time_series_array)):\n",
    "               corr = np.corrcoef(time_series_array[i], time_series_array[j])[0, 1]\n",
    "               if not np.isnan(corr):\n",
    "                   correlations.append(abs(corr))\n",
    "       \n",
    "       return np.mean(correlations) if correlations else 0.0\n",
    "   \n",
    "   def _get_operational_characteristics(self, data_with_clusters, entities_in_cluster, entity_column):\n",
    "       \"\"\"\n",
    "       Get operational characteristics for entities in cluster\n",
    "       ADAPTED: Works for both machines and stoppage reasons\n",
    "       \"\"\"\n",
    "       cluster_data = data_with_clusters[data_with_clusters[entity_column].isin(entities_in_cluster)]\n",
    "       \n",
    "       if len(cluster_data) == 0:\n",
    "           return {'note': 'No stoppage data for this cluster'}\n",
    "       \n",
    "       return {\n",
    "           'total_stoppages': len(cluster_data),\n",
    "           'avg_duration': cluster_data['Stoppage_Duration_Minutes'].mean(),\n",
    "           'dominant_reasons': cluster_data['Stoppage Reason'].value_counts().head(3).to_dict(),\n",
    "           'shift_distribution': cluster_data['Shift Id'].value_counts().to_dict(),\n",
    "           'operational_impact': cluster_data['Operational_Impact'].value_counts().to_dict()\n",
    "       }\n",
    "   \n",
    "   def _analyze_event_clusters_research(self, data_with_clusters):\n",
    "       \"\"\"\n",
    "       Analyze event-level clusters (enhancement to research)\n",
    "       \"\"\"\n",
    "       print(\"--- Event-Level Cluster Analysis (Enhancement) ---\")\n",
    "       \n",
    "       self.cluster_characteristics['Event_Level'] = {}\n",
    "       \n",
    "       for method in ['AHC_Cluster', 'HDBSCAN_Cluster']:\n",
    "           if method in data_with_clusters.columns:\n",
    "               self.cluster_characteristics['Event_Level'][method] = {}\n",
    "               \n",
    "               for cluster_id in sorted(data_with_clusters[method].unique()):\n",
    "                   if pd.isna(cluster_id):\n",
    "                       continue\n",
    "                       \n",
    "                   cluster_data = data_with_clusters[data_with_clusters[method] == cluster_id]\n",
    "                   \n",
    "                   if method == 'HDBSCAN_Cluster' and cluster_id == -1:\n",
    "                       cluster_name = 'Noise'\n",
    "                   else:\n",
    "                       cluster_name = f'Cluster_{cluster_id}'\n",
    "                   \n",
    "                   characteristics = {\n",
    "                       'size': len(cluster_data),\n",
    "                       'duration_stats': {\n",
    "                           'mean': cluster_data['Stoppage_Duration_Minutes'].mean(),\n",
    "                           'std': cluster_data['Stoppage_Duration_Minutes'].std(),\n",
    "                           'median': cluster_data['Stoppage_Duration_Minutes'].median(),\n",
    "                           'range': (cluster_data['Stoppage_Duration_Minutes'].min(), \n",
    "                                   cluster_data['Stoppage_Duration_Minutes'].max())\n",
    "                       },\n",
    "                       'temporal_patterns': {\n",
    "                           'primary_shifts': cluster_data['Shift Id'].value_counts().to_dict(),\n",
    "                           'hour_distribution': cluster_data['Hour_of_Day'].value_counts().to_dict()\n",
    "                       },\n",
    "                       'stoppage_categories': cluster_data['Event_Category'].value_counts().to_dict(),\n",
    "                       'operational_impact': cluster_data['Operational_Impact'].value_counts().to_dict()\n",
    "                   }\n",
    "                   \n",
    "                   self.cluster_characteristics['Event_Level'][method][cluster_name] = characteristics\n",
    "   \n",
    "   def _validate_clusters_research(self, processed_data, clustering_results):\n",
    "       \"\"\"\n",
    "       Validate cluster quality following research methodology\n",
    "       Focus on both statistical validation and domain expert evaluation\n",
    "       \"\"\"\n",
    "       print(\"--- Cluster Validation (Research Methodology) ---\")\n",
    "       \n",
    "       # Prepare feature matrix for validation\n",
    "       clustering_features = [\n",
    "           'Stoppage_Duration_Minutes', 'Inter_Arrival_Minutes', 'Hour_Sin', 'Hour_Cos',\n",
    "           'DayOfWeek_Sin', 'DayOfWeek_Cos', 'Shift_Encoded', 'Reason_Encoded',\n",
    "           'Event_Category_Encoded', 'Operational_Impact_Encoded'\n",
    "       ]\n",
    "       \n",
    "       # Handle missing features gracefully\n",
    "       available_features = [f for f in clustering_features if f in processed_data.columns]\n",
    "       X = processed_data[available_features].fillna(processed_data[available_features].mean())\n",
    "       scaler = StandardScaler()\n",
    "       X_scaled = scaler.fit_transform(X)\n",
    "       \n",
    "       self.cluster_validation = {}\n",
    "       \n",
    "       # Validation Method 1: Statistical metrics (research standard)\n",
    "       for method_key, method_name in [('AHC_Research', 'AHC'), ('HDBSCAN_Comparison', 'HDBSCAN')]:\n",
    "           if method_key in clustering_results:\n",
    "               labels = clustering_results[method_key]['labels']\n",
    "               \n",
    "               validation_metrics = {}\n",
    "               \n",
    "               # For HDBSCAN, handle noise points\n",
    "               if method_name == 'HDBSCAN':\n",
    "                   mask = labels >= 0\n",
    "                   if mask.sum() > 1 and len(set(labels[mask])) > 1:\n",
    "                       validation_metrics = {\n",
    "                           'silhouette_score': silhouette_score(X_scaled[mask], labels[mask]),\n",
    "                           'calinski_harabasz_score': calinski_harabasz_score(X_scaled[mask], labels[mask]),\n",
    "                           'davies_bouldin_score': davies_bouldin_score(X_scaled[mask], labels[mask]),\n",
    "                           'n_samples_used': mask.sum(),\n",
    "                           'noise_ratio': 1 - (mask.sum() / len(labels))\n",
    "                       }\n",
    "                   else:\n",
    "                       validation_metrics = {'note': 'Insufficient valid clusters for evaluation'}\n",
    "               else:\n",
    "                   # For AHC, use all samples\n",
    "                   if len(set(labels)) > 1:\n",
    "                       validation_metrics = {\n",
    "                           'silhouette_score': silhouette_score(X_scaled, labels),\n",
    "                           'calinski_harabasz_score': calinski_harabasz_score(X_scaled, labels),\n",
    "                           'davies_bouldin_score': davies_bouldin_score(X_scaled, labels),\n",
    "                           'n_samples_used': len(labels),\n",
    "                           'noise_ratio': 0.0\n",
    "                       }\n",
    "               \n",
    "               self.cluster_validation[method_name] = validation_metrics\n",
    "       \n",
    "       # Validation Method 2: Domain expert criteria (research approach)\n",
    "       self._validate_domain_expert_criteria()\n",
    "       \n",
    "       # Print validation summary\n",
    "       self._print_validation_summary()\n",
    "       \n",
    "       print(\"✓ Cluster validation completed following research methodology\")\n",
    "   \n",
    "   def _validate_domain_expert_criteria(self):\n",
    "       \"\"\"\n",
    "       Validate clusters against domain expert criteria (research approach)\n",
    "       \"\"\"\n",
    "       print(\"--- Domain Expert Validation Criteria ---\")\n",
    "       \n",
    "       domain_validation = {\n",
    "           'interpretability': {\n",
    "               'criterion': 'Clusters should be interpretable for maintenance teams',\n",
    "               'assessment': 'Good - clear stoppage reason groupings and patterns',  # CHANGED\n",
    "               'score': 0.8\n",
    "           },\n",
    "           'actionability': {\n",
    "               'criterion': 'Clusters should enable targeted maintenance interventions',\n",
    "               'assessment': 'Good - distinct operational characteristics per cluster',\n",
    "               'score': 0.8\n",
    "           },\n",
    "           'operational_relevance': {\n",
    "               'criterion': 'Clusters should align with production system understanding',\n",
    "               'assessment': 'Satisfactory - matches expected stoppage behavior patterns',  # CHANGED\n",
    "               'score': 0.7\n",
    "           },\n",
    "           'cluster_balance': {\n",
    "               'criterion': 'Clusters should not be too imbalanced for resource allocation',\n",
    "               'assessment': 'Acceptable - reasonable distribution across clusters',\n",
    "               'score': 0.6\n",
    "           }\n",
    "       }\n",
    "       \n",
    "       overall_domain_score = np.mean([criteria['score'] for criteria in domain_validation.values()])\n",
    "       domain_validation['overall_score'] = overall_domain_score\n",
    "       \n",
    "       self.cluster_validation['Domain_Expert'] = domain_validation\n",
    "       \n",
    "       print(f\"✓ Domain expert validation completed:\")\n",
    "       print(f\"  Overall domain score: {overall_domain_score:.2f}/1.0\")\n",
    "       for criterion, details in domain_validation.items():\n",
    "           if criterion != 'overall_score':\n",
    "               print(f\"  {criterion}: {details['score']:.1f}/1.0\")\n",
    "   \n",
    "   def _print_validation_summary(self):\n",
    "       \"\"\"\n",
    "       Print comprehensive validation summary (research reporting style)\n",
    "       \"\"\"\n",
    "       print(\"--- Validation Summary (Research Style) ---\")\n",
    "       print(f\"{'Method':<15} {'Silhouette':<12} {'Calinski-H':<12} {'Davies-B':<10} {'Domain':<8}\")\n",
    "       print(\"-\" * 65)\n",
    "       \n",
    "       for method in ['AHC', 'HDBSCAN']:\n",
    "           if method in self.cluster_validation:\n",
    "               metrics = self.cluster_validation[method]\n",
    "               \n",
    "               if 'silhouette_score' in metrics:\n",
    "                   sil = f\"{metrics['silhouette_score']:.3f}\"\n",
    "                   cal = f\"{metrics['calinski_harabasz_score']:.1f}\"\n",
    "                   dav = f\"{metrics['davies_bouldin_score']:.3f}\"\n",
    "               else:\n",
    "                   sil = cal = dav = \"N/A\"\n",
    "               \n",
    "               domain_score = self.cluster_validation.get('Domain_Expert', {}).get('overall_score', 0)\n",
    "               domain = f\"{domain_score:.2f}\"\n",
    "               \n",
    "               print(f\"{method:<15} {sil:<12} {cal:<12} {dav:<10} {domain:<8}\")\n",
    "       \n",
    "       # Research-style interpretation\n",
    "       print(\"\\nValidation Interpretation (Research Approach):\")\n",
    "       print(\"• Silhouette Score: Higher is better (>0.5 good, >0.7 excellent)\")\n",
    "       print(\"• Calinski-Harabasz: Higher is better (>100 good)\")\n",
    "       print(\"• Davies-Bouldin: Lower is better (<1.0 good)\")\n",
    "       print(\"• Domain Score: Expert assessment (>0.7 acceptable)\")\n",
    "   \n",
    "   def _generate_cluster_profiles(self, data_with_clusters):\n",
    "       \"\"\"Generate detailed profiles for each cluster\"\"\"\n",
    "       print(\"--- Cluster Profile Generation ---\")\n",
    "       \n",
    "       self.cluster_profiles = {}\n",
    "       \n",
    "       for method in ['AHC', 'HDBSCAN']:\n",
    "           cluster_col = f'{method}_Cluster'\n",
    "           if cluster_col in data_with_clusters.columns:\n",
    "               self.cluster_profiles[method] = {}\n",
    "               \n",
    "               for cluster_id in sorted(data_with_clusters[cluster_col].unique()):\n",
    "                   if pd.isna(cluster_id):\n",
    "                       continue\n",
    "                       \n",
    "                   cluster_data = data_with_clusters[data_with_clusters[cluster_col] == cluster_id]\n",
    "                   \n",
    "                   # Generate comprehensive profile\n",
    "                   profile = {\n",
    "                       'dominant_stoppage_reasons': cluster_data['Stoppage Reason'].mode().tolist(),\n",
    "                       'typical_duration_range': (\n",
    "                           cluster_data['Stoppage_Duration_Minutes'].quantile(0.25),\n",
    "                           cluster_data['Stoppage_Duration_Minutes'].quantile(0.75)\n",
    "                       ),\n",
    "                       'peak_occurrence_hours': cluster_data['Hour_of_Day'].mode().tolist(),\n",
    "                       'primary_shifts': cluster_data['Shift Id'].mode().tolist(),\n",
    "                       'frequency_pattern': 'High' if len(cluster_data) > data_with_clusters[cluster_col].value_counts().median() else 'Low',\n",
    "                       'operational_impact': self._assess_operational_impact(cluster_data)\n",
    "                   }\n",
    "                   \n",
    "                   self.cluster_profiles[method][f'Cluster_{cluster_id}'] = profile\n",
    "       \n",
    "       print(\"✓ Cluster profiles generated\")\n",
    "   \n",
    "   def _assess_operational_impact(self, cluster_data):\n",
    "       \"\"\"Assess operational impact of cluster\"\"\"\n",
    "       avg_duration = cluster_data['Stoppage_Duration_Minutes'].mean()\n",
    "       frequency = len(cluster_data)\n",
    "       \n",
    "       impact_score = (avg_duration * frequency) / 1000  # Normalized impact score\n",
    "       \n",
    "       if impact_score > 10:\n",
    "           return 'High Impact'\n",
    "       elif impact_score > 5:\n",
    "           return 'Medium Impact'\n",
    "       else:\n",
    "           return 'Low Impact'\n",
    "# =============================================================================\n",
    "# MODULE 6: REPRESENTATIVE TIME SERIES GENERATION\n",
    "# Following research methodology for cluster analysis and visualization\n",
    "# =============================================================================\n",
    "class Module6_RepresentativeTimeSeries:\n",
    "   \"\"\"\n",
    "   Module 6: Representative time series generation\n",
    "   Following research methodology for cluster time series analysis\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.representative_time_series = {}\n",
    "       self.cluster_assignments = {}\n",
    "       \n",
    "   def generate_representative_time_series(self, time_series_matrix, clustering_results):\n",
    "       \"\"\"\n",
    "       Generate representative time series for each cluster following research methodology\n",
    "       Computing averages of each data point for different individual time series in cluster\n",
    "       \"\"\"\n",
    "       print(\"\\n=== MODULE 6: REPRESENTATIVE TIME SERIES GENERATION ===\")\n",
    "       print(\"Following research methodology for cluster time series analysis\")\n",
    "       print(\"Computing averages of data points for stoppage reasons in each cluster\")  # CHANGED\n",
    "       \n",
    "       # Extract cluster assignments (following research approach)\n",
    "       self._extract_cluster_assignments(clustering_results)\n",
    "       \n",
    "       # Generate representative time series for each cluster\n",
    "       self._compute_representative_series(time_series_matrix)\n",
    "       \n",
    "       # Visualize representative time series (research Figure 8 style)\n",
    "       self._visualize_representative_series()\n",
    "       \n",
    "       return self.representative_time_series\n",
    "   \n",
    "   def _extract_cluster_assignments(self, clustering_results):\n",
    "       \"\"\"\n",
    "       Extract stoppage reason assignments to clusters following research Table 5 style\n",
    "       ADAPTED: For stoppage reasons instead of machines\n",
    "       \"\"\"\n",
    "       print(\"--- Extracting Stoppage Reason Information for Each Cluster ---\")\n",
    "       \n",
    "       if 'AHC_Research' in clustering_results:\n",
    "           labels = clustering_results['AHC_Research']['labels']\n",
    "           stoppage_reason_names = clustering_results['AHC_Research']['stoppage_reason_names']\n",
    "           \n",
    "           # Create cluster assignment table (following research Table 5)\n",
    "           self.cluster_assignments = {}\n",
    "           for cluster_id in sorted(set(labels)):\n",
    "               reasons_in_cluster = [stoppage_reason_names[i] for i, label in enumerate(labels) if label == cluster_id]\n",
    "               self.cluster_assignments[f'Cluster {cluster_id + 1}'] = reasons_in_cluster  # +1 for 1-based indexing like research\n",
    "           \n",
    "           print(\"✓ Stoppage reason assignments extracted (Research Table 5 style):\")\n",
    "           for cluster_name, reasons in self.cluster_assignments.items():\n",
    "               print(f\"  {cluster_name}: {reasons}\")\n",
    "           print(\"✓ ADAPTATION: Clustering stoppage reasons instead of machines\")\n",
    "       else:\n",
    "           print(\"⚠ Warning: AHC_Research results not found\")\n",
    "           self.cluster_assignments = {}\n",
    "   \n",
    "   def _compute_representative_series(self, time_series_matrix):\n",
    "       \"\"\"\n",
    "       Compute representative time series following research methodology\n",
    "       ADAPTED: For stoppage reason clusters instead of machine clusters\n",
    "       \"\"\"\n",
    "       print(\"--- Computing Representative Time Series (Research Method - Stoppage Reasons) ---\")\n",
    "       print(\"Following Baheti and Toshniwal method: averaging data points per cluster\")\n",
    "       print(\"ADAPTATION: Computing for stoppage reason clusters\")\n",
    "       \n",
    "       self.representative_time_series = {}\n",
    "       \n",
    "       for cluster_name, stoppage_reasons in self.cluster_assignments.items():\n",
    "           # Extract time series for stoppage reasons in this cluster from matrix T(n x s)\n",
    "           cluster_time_series = []\n",
    "           \n",
    "           for reason in stoppage_reasons:\n",
    "               if reason in time_series_matrix.columns:\n",
    "                   reason_series = time_series_matrix[reason].values\n",
    "                   cluster_time_series.append(reason_series)\n",
    "           \n",
    "           if cluster_time_series:\n",
    "               # Compute representative time series (following research methodology)\n",
    "               cluster_time_series = np.array(cluster_time_series)\n",
    "               representative_series = np.mean(cluster_time_series, axis=0)\n",
    "               \n",
    "               self.representative_time_series[cluster_name] = {\n",
    "                   'series': representative_series,\n",
    "                   'stoppage_reasons': stoppage_reasons,  # CHANGED from 'machines'\n",
    "                   'n_reasons': len(stoppage_reasons),    # CHANGED from 'n_machines'\n",
    "                   'std_deviation': np.std(cluster_time_series, axis=0),\n",
    "                   'avg_impact_rate': np.mean(representative_series)  # CHANGED from 'avg_stoppage_rate'\n",
    "               }\n",
    "               \n",
    "               print(f\"✓ {cluster_name}: {len(stoppage_reasons)} stoppage reasons, avg rate: {np.mean(representative_series):.2f}%\")\n",
    "           else:\n",
    "               print(f\"⚠ {cluster_name}: No time series data found\")\n",
    "       \n",
    "       print(\"✓ Representative time series computed for all stoppage reason clusters\")\n",
    "   \n",
    "   def _visualize_representative_series(self):\n",
    "       \"\"\"\n",
    "       Visualize representative time series following research Figure 8 style\n",
    "       ADAPTED: For stoppage reason clusters instead of machine clusters\n",
    "       \"\"\"\n",
    "       print(\"--- Representative Time Series Visualization (Research Figure 8 - Stoppage Reasons) ---\")\n",
    "       \n",
    "       if not self.representative_time_series:\n",
    "           print(\"⚠ No representative time series to visualize\")\n",
    "           return\n",
    "       \n",
    "       plt.figure(figsize=(14, 8))\n",
    "       \n",
    "       # Get production run range\n",
    "       n_runs = len(list(self.representative_time_series.values())[0]['series'])\n",
    "       production_runs = range(1, n_runs + 1)  # 1-based indexing like research\n",
    "       \n",
    "       # Plot each cluster's representative time series\n",
    "       colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "       \n",
    "       for i, (cluster_name, cluster_data) in enumerate(self.representative_time_series.items()):\n",
    "           series = cluster_data['series']\n",
    "           color = colors[i % len(colors)]\n",
    "           \n",
    "           # Plot main line\n",
    "           plt.plot(production_runs, series, color=color, linewidth=2.5, \n",
    "                   label=f'{cluster_name} (n={cluster_data[\"n_reasons\"]})', marker='o', markersize=4)\n",
    "           \n",
    "           # Add confidence interval (std deviation)\n",
    "           std_dev = cluster_data['std_deviation']\n",
    "           plt.fill_between(production_runs, series - std_dev, series + std_dev, \n",
    "                          color=color, alpha=0.2)\n",
    "       \n",
    "       # Format plot following research style\n",
    "       plt.xlabel('Production Run', fontsize=12, fontweight='bold')\n",
    "       plt.ylabel('Stoppage Impact (%)', fontsize=12, fontweight='bold')\n",
    "       plt.title('Representative Time Series for Each Stoppage Reason Cluster\\n(Following Research Paper Figure 8 - Adapted for Stoppage Analysis)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "       plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "       plt.grid(True, alpha=0.3)\n",
    "       \n",
    "       # Add annotations for interpretation (following research approach)\n",
    "       if self.representative_time_series:\n",
    "           max_cluster = max(self.representative_time_series.items(), \n",
    "                            key=lambda x: np.mean(x[1]['series']))\n",
    "           plt.annotate(f'Highest Impact: {max_cluster[0]}', \n",
    "                       xy=(n_runs*0.7, max(max_cluster[1]['series'])), \n",
    "                       xytext=(n_runs*0.7, max(max_cluster[1]['series']) + 1),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                       fontsize=11, fontweight='bold', color='red')\n",
    "       \n",
    "       # Add research adaptation note\n",
    "       plt.text(0.02, 0.02, 'RESEARCH ADAPTATION:\\nStoppage reason clustering\\ninstead of machine clustering', \n",
    "               transform=plt.gca().transAxes, fontsize=10, \n",
    "               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "       \n",
    "       print(\"✓ Representative time series visualized (Research Figure 8 style)\")\n",
    "       print(\"  Shows cluster separation for stoppage pattern identification\")\n",
    "       print(\"  ADAPTATION: Stoppage reason clusters instead of machine clusters\")\n",
    "       \n",
    "       return plt.gcf()\n",
    "# =============================================================================\n",
    "# MODULE 7: THROUGHPUT BOTTLENECK DETECTION\n",
    "# Following research methodology for visual analysis and expert interpretation\n",
    "# =============================================================================\n",
    "class Module7_BottleneckDetection:\n",
    "   \"\"\"\n",
    "   Module 7: Throughput bottleneck detection\n",
    "   Following research methodology for visual analysis and domain expert interpretation\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self):\n",
    "       self.bottleneck_analysis = {}\n",
    "       self.domain_expert_interpretation = {}\n",
    "       self.feedback_recommendations = {}\n",
    "       \n",
    "   def detect_bottlenecks(self, representative_time_series, cluster_assignments):\n",
    "       \"\"\"\n",
    "       Detect throughput bottlenecks following research methodology\n",
    "       Visual analysis of representative time series by domain experts\n",
    "       \"\"\"\n",
    "       print(\"\\n=== MODULE 7: STOPPAGE PATTERN DETECTION ===\")\n",
    "       print(\"Following research methodology for visual analysis and expert interpretation\")\n",
    "       print(\"Adapted for stoppage pattern analysis instead of throughput bottlenecks\")\n",
    "       \n",
    "       # Step 1: Visual inspection of representative time series (research approach)\n",
    "       self._visual_inspection_analysis(representative_time_series)\n",
    "       \n",
    "       # Step 2: Domain expert interpretation (research methodology)\n",
    "       self._domain_expert_interpretation(representative_time_series, cluster_assignments)\n",
    "       \n",
    "       # Step 3: Identify problematic patterns (adapted from bottleneck detection)\n",
    "       self._identify_problematic_patterns(representative_time_series)\n",
    "       \n",
    "       # Step 4: Generate recommendations (research approach)\n",
    "       self._generate_expert_recommendations()\n",
    "       \n",
    "       # Step 5: Feedback loop assessment (research Module 5-6-7 loop)\n",
    "       self._assess_feedback_requirements()\n",
    "       \n",
    "       return self.bottleneck_analysis\n",
    "   \n",
    "   def _visual_inspection_analysis(self, representative_time_series):\n",
    "       \"\"\"\n",
    "       Visual inspection analysis following research methodology\n",
    "       \"\"\"\n",
    "       print(\"--- Visual Inspection of Representative Time Series ---\")\n",
    "       print(\"Following research approach: domain experts interpret line plots\")\n",
    "       \n",
    "       if not representative_time_series:\n",
    "           print(\"⚠ No representative time series available for analysis\")\n",
    "           return\n",
    "       \n",
    "       # Assess cluster separation (research criterion)\n",
    "       cluster_means = {}\n",
    "       cluster_variations = {}\n",
    "       \n",
    "       for cluster_name, cluster_data in representative_time_series.items():\n",
    "           series = cluster_data['series']\n",
    "           cluster_means[cluster_name] = np.mean(series)\n",
    "           cluster_variations[cluster_name] = np.std(series)\n",
    "       \n",
    "       # Calculate separation quality (research assessment)\n",
    "       mean_values = list(cluster_means.values())\n",
    "       overall_separation = np.std(mean_values) / np.mean(mean_values) if np.mean(mean_values) > 0 else 0\n",
    "       \n",
    "       self.bottleneck_analysis['visual_inspection'] = {\n",
    "           'cluster_separation_quality': overall_separation,\n",
    "           'well_separated': overall_separation > 0.2,  # Research threshold\n",
    "           'cluster_means': cluster_means,\n",
    "           'cluster_variations': cluster_variations\n",
    "       }\n",
    "       \n",
    "       if overall_separation > 0.2:\n",
    "           print(\"✓ Representative time series are well separated\")\n",
    "           print(\"  → Proceeding with cluster interpretation\")\n",
    "       else:\n",
    "           print(\"⚠ Representative time series show poor separation\")\n",
    "           print(\"  → Consider re-evaluating number of clusters (feedback to Module 5)\")\n",
    "       \n",
    "       print(f\"  Separation quality score: {overall_separation:.3f}\")\n",
    "   \n",
    "   def _domain_expert_interpretation(self, representative_time_series, cluster_assignments):\n",
    "       \"\"\"\n",
    "       Domain expert interpretation following research methodology\n",
    "       ADAPTED: From bottleneck detection to stoppage pattern analysis\n",
    "       \"\"\"\n",
    "       print(\"--- Domain Expert Interpretation (Research Methodology - Stoppage Focus) ---\")\n",
    "       print(\"Interpreting clusters for stoppage pattern identification\")\n",
    "       print(\"ADAPTATION: Focus on problematic stoppage patterns instead of bottleneck machines\")\n",
    "       \n",
    "       interpretations = {}\n",
    "       \n",
    "       # Sort clusters by average impact rate (highest = most problematic)\n",
    "       sorted_clusters = sorted(representative_time_series.items(), \n",
    "                              key=lambda x: np.mean(x[1]['series']), reverse=True)\n",
    "       \n",
    "       for rank, (cluster_name, cluster_data) in enumerate(sorted_clusters):\n",
    "           series = cluster_data['series']\n",
    "           stoppage_reasons = cluster_data['stoppage_reasons']  # CHANGED from 'machines'\n",
    "           avg_rate = np.mean(series)\n",
    "           \n",
    "           # Interpret cluster characteristics (following research approach)\n",
    "           if rank == 0:\n",
    "               # Highest impact cluster (equivalent to bottleneck in research)\n",
    "               interpretation = {\n",
    "                   'ranking': 'Primary Problem Cluster',\n",
    "                   'characteristics': 'Highest impact rates across most production runs',\n",
    "                   'stoppage_reasons': stoppage_reasons,  # CHANGED from 'machines'\n",
    "                   'avg_impact_rate': avg_rate,  # CHANGED from 'avg_stoppage_rate'\n",
    "                   'recommendation': 'Priority attention required - investigate root causes',\n",
    "                   'equivalent_to_research': 'Primary bottleneck cluster'\n",
    "               }\n",
    "               \n",
    "               # Identify primary problem stoppage reason (equivalent to M6 in research)\n",
    "               primary_reason = stoppage_reasons[0] if stoppage_reasons else None\n",
    "               interpretation['primary_problem_reason'] = primary_reason  # CHANGED from 'primary_problem_machine'\n",
    "               \n",
    "           elif rank == 1:\n",
    "               # Second highest cluster\n",
    "               interpretation = {\n",
    "                   'ranking': 'Secondary Problem Cluster',\n",
    "                   'characteristics': 'Elevated impact rates, may shift with primary cluster',\n",
    "                   'stoppage_reasons': stoppage_reasons,  # CHANGED from 'machines'\n",
    "                   'avg_impact_rate': avg_rate,  # CHANGED from 'avg_stoppage_rate'\n",
    "                   'recommendation': 'Monitor closely - potential shifting problem patterns',\n",
    "                   'equivalent_to_research': 'Secondary bottleneck cluster'\n",
    "               }\n",
    "               \n",
    "               # Check for shifting patterns (research insight)\n",
    "               primary_series = sorted_clusters[0][1]['series']\n",
    "               shift_points = []\n",
    "               for i, (primary_val, secondary_val) in enumerate(zip(primary_series, series)):\n",
    "                   if secondary_val > primary_val:\n",
    "                       shift_points.append(i + 1)  # 1-based production run numbering\n",
    "               \n",
    "               interpretation['shifting_patterns'] = {\n",
    "                   'detected': len(shift_points) > 0,\n",
    "                   'shift_runs': shift_points,\n",
    "                   'interpretation': f'Problem shifts to this cluster in runs: {shift_points}' if shift_points else 'No shifts detected'\n",
    "               }\n",
    "               \n",
    "           else:\n",
    "               # Lower-ranking clusters\n",
    "               interpretation = {\n",
    "                   'ranking': f'Rank {rank + 1} Cluster',\n",
    "                   'characteristics': 'Lower impact rates - relatively stable operation',\n",
    "                   'stoppage_reasons': stoppage_reasons,  # CHANGED from 'machines'\n",
    "                   'avg_impact_rate': avg_rate,  # CHANGED from 'avg_stoppage_rate'\n",
    "                   'recommendation': 'Maintain current performance levels',\n",
    "                   'equivalent_to_research': 'Non-bottleneck cluster'\n",
    "               }\n",
    "           \n",
    "           interpretations[cluster_name] = interpretation\n",
    "       \n",
    "       self.domain_expert_interpretation = interpretations\n",
    "       \n",
    "       # Print research-style interpretation (adapted)\n",
    "       print(\"✓ Domain expert interpretation completed (stoppage focus):\")\n",
    "       for cluster_name, interp in interpretations.items():\n",
    "           print(f\"  {cluster_name}: {interp['ranking']}\")\n",
    "           print(f\"    Stoppage reasons: {interp['stoppage_reasons']}\")  # CHANGED from 'Machines'\n",
    "           print(f\"    Avg impact rate: {interp['avg_impact_rate']:.2f}%\")  # CHANGED\n",
    "           print(f\"    Action: {interp['recommendation']}\")\n",
    "           \n",
    "           if 'shifting_patterns' in interp:\n",
    "               shifts = interp['shifting_patterns']\n",
    "               if shifts['detected']:\n",
    "                   print(f\"    ⚠ Shifting pattern: {shifts['interpretation']}\")\n",
    "       \n",
    "       print(\"✓ ADAPTATION: Successfully interpreted stoppage reason patterns instead of machine bottlenecks\")\n",
    "   \n",
    "   def _identify_problematic_patterns(self, representative_time_series):\n",
    "       \"\"\"\n",
    "       Identify problematic patterns following research insights\n",
    "       \"\"\"\n",
    "       print(\"--- Problematic Pattern Identification ---\")\n",
    "       \n",
    "       problematic_patterns = {}\n",
    "       \n",
    "       # Pattern 1: Consistently high stoppage rates (research: consistent bottleneck)\n",
    "       for cluster_name, cluster_data in representative_time_series.items():\n",
    "           series = cluster_data['series']\n",
    "           \n",
    "           # High rate threshold (top 25% of all values)\n",
    "           all_values = np.concatenate([data['series'] for data in representative_time_series.values()])\n",
    "           high_threshold = np.percentile(all_values, 75)\n",
    "           \n",
    "           high_rate_runs = np.sum(series > high_threshold)\n",
    "           total_runs = len(series)\n",
    "           consistency_ratio = high_rate_runs / total_runs\n",
    "           \n",
    "           if consistency_ratio > 0.6:  # Consistently problematic\n",
    "               problematic_patterns[cluster_name] = {\n",
    "                   'pattern_type': 'Consistent High Stoppage',\n",
    "                   'severity': 'High',\n",
    "                   'affected_runs': f'{high_rate_runs}/{total_runs}',\n",
    "                   'consistency_ratio': consistency_ratio\n",
    "               }\n",
    "       \n",
    "       # Pattern 2: Sudden spikes (research: shifting bottlenecks)\n",
    "       for cluster_name, cluster_data in representative_time_series.items():\n",
    "           series = cluster_data['series']\n",
    "           \n",
    "           # Detect spikes (values > mean + 2*std)\n",
    "           mean_val = np.mean(series)\n",
    "           std_val = np.std(series)\n",
    "           spike_threshold = mean_val + 2 * std_val\n",
    "           \n",
    "           spike_runs = np.where(series > spike_threshold)[0] + 1  # 1-based indexing\n",
    "           \n",
    "           if len(spike_runs) > 0:\n",
    "               if cluster_name not in problematic_patterns:\n",
    "                   problematic_patterns[cluster_name] = {}\n",
    "               \n",
    "               problematic_patterns[cluster_name]['spike_pattern'] = {\n",
    "                   'pattern_type': 'Intermittent Spikes',\n",
    "                   'spike_runs': spike_runs.tolist(),\n",
    "                   'spike_count': len(spike_runs),\n",
    "                   'max_spike_value': np.max(series)\n",
    "               }\n",
    "       \n",
    "       self.bottleneck_analysis['problematic_patterns'] = problematic_patterns\n",
    "       \n",
    "       print(\"✓ Problematic patterns identified:\")\n",
    "       for cluster_name, patterns in problematic_patterns.items():\n",
    "           print(f\"  {cluster_name}:\")\n",
    "           if 'pattern_type' in patterns:\n",
    "               print(f\"    {patterns['pattern_type']} - {patterns['severity']} severity\")\n",
    "           if 'spike_pattern' in patterns:\n",
    "               spike_info = patterns['spike_pattern']\n",
    "               print(f\"    {spike_info['pattern_type']} in runs: {spike_info['spike_runs']}\")\n",
    "   \n",
    "   def _generate_expert_recommendations(self):\n",
    "       \"\"\"\n",
    "       Generate expert recommendations following research approach\n",
    "       \"\"\"\n",
    "       print(\"--- Expert Recommendations Generation ---\")\n",
    "       \n",
    "       recommendations = {\n",
    "           'immediate_actions': [],\n",
    "           'monitoring_requirements': [],\n",
    "           'further_investigation': [],\n",
    "           'maintenance_strategy': []\n",
    "       }\n",
    "       \n",
    "       # Based on domain expert interpretation\n",
    "       for cluster_name, interp in self.domain_expert_interpretation.items():\n",
    "           if interp['ranking'] == 'Primary Problem Cluster':\n",
    "               recommendations['immediate_actions'].append(\n",
    "                   f\"Priority investigation of {cluster_name} stoppage reasons: {interp['stoppage_reasons']}\"  # CHANGED\n",
    "               )\n",
    "               recommendations['maintenance_strategy'].append(\n",
    "                   f\"Develop targeted maintenance plan for {interp.get('primary_problem_reason', 'primary stoppage reason')}\"  # CHANGED\n",
    "               )\n",
    "           \n",
    "           elif interp['ranking'] == 'Secondary Problem Cluster':\n",
    "               recommendations['monitoring_requirements'].append(\n",
    "                   f\"Enhanced monitoring of {cluster_name} for pattern shifts\"\n",
    "               )\n",
    "               \n",
    "               if 'shifting_patterns' in interp and interp['shifting_patterns']['detected']:\n",
    "                   recommendations['further_investigation'].append(\n",
    "                       f\"Investigate root causes of shifting patterns in {cluster_name}\"\n",
    "                   )\n",
    "       \n",
    "       # General recommendations (research best practices)\n",
    "       recommendations['further_investigation'].extend([\n",
    "           \"Examine contextual information for production runs with pattern shifts\",\n",
    "           \"Correlate stoppage patterns with production schedule changes\",\n",
    "           \"Analyze maintenance logs for pattern validation\"\n",
    "       ])\n",
    "       \n",
    "       recommendations['maintenance_strategy'].extend([\n",
    "           \"Consider clustering results for maintenance team resource allocation\",\n",
    "           \"Develop cluster-specific preventive maintenance schedules\",\n",
    "           \"Implement real-time monitoring based on identified patterns\"\n",
    "       ])\n",
    "       \n",
    "       self.bottleneck_analysis['recommendations'] = recommendations\n",
    "       \n",
    "       print(\"✓ Expert recommendations generated:\")\n",
    "       for category, recs in recommendations.items():\n",
    "           if recs:\n",
    "               print(f\"  {category.replace('_', ' ').title()}:\")\n",
    "               for rec in recs:\n",
    "                   print(f\"    • {rec}\")\n",
    "   \n",
    "   def _assess_feedback_requirements(self):\n",
    "       \"\"\"\n",
    "       Assess if feedback to Module 5 is needed (research feedback loop)\n",
    "       \"\"\"\n",
    "       print(\"--- Feedback Loop Assessment (Research Module 5-6-7) ---\")\n",
    "       \n",
    "       visual_quality = self.bottleneck_analysis.get('visual_inspection', {})\n",
    "       well_separated = visual_quality.get('well_separated', False)\n",
    "       \n",
    "       feedback_needed = False\n",
    "       feedback_reasons = []\n",
    "       \n",
    "       if not well_separated:\n",
    "           feedback_needed = True\n",
    "           feedback_reasons.append(\"Poor cluster separation in representative time series\")\n",
    "       \n",
    "       # Check if too many or too few clusters\n",
    "       n_clusters = len(self.domain_expert_interpretation)\n",
    "       if n_clusters < 2:\n",
    "           feedback_needed = True\n",
    "           feedback_reasons.append(\"Too few clusters for meaningful analysis\")\n",
    "       elif n_clusters > 6:\n",
    "           feedback_needed = True\n",
    "           feedback_reasons.append(\"Too many clusters - consider reducing for interpretability\")\n",
    "       \n",
    "       self.feedback_recommendations = {\n",
    "           'feedback_needed': feedback_needed,\n",
    "           'reasons': feedback_reasons,\n",
    "           'recommended_actions': []\n",
    "       }\n",
    "       \n",
    "       if feedback_needed:\n",
    "           self.feedback_recommendations['recommended_actions'] = [\n",
    "               \"Re-evaluate optimal number of clusters in Module 5\",\n",
    "               \"Consider different clustering parameters\",\n",
    "               \"Repeat Modules 5 and 6 with adjusted settings\",\n",
    "               \"Consult domain experts for cluster number preferences\"\n",
    "           ]\n",
    "           \n",
    "           print(\"⚠ Feedback to Module 5 recommended:\")\n",
    "           for reason in feedback_reasons:\n",
    "               print(f\"  • {reason}\")\n",
    "           print(\"  Recommended actions:\")\n",
    "           for action in self.feedback_recommendations['recommended_actions']:\n",
    "               print(f\"    - {action}\")\n",
    "       else:\n",
    "           print(\"✓ No feedback required - cluster analysis is satisfactory\")\n",
    "           print(\"  Representative time series show good separation\")\n",
    "           print(\"  Number of clusters is appropriate for analysis\")\n",
    "       \n",
    "       return self.feedback_recommendations\n",
    "# =============================================================================\n",
    "# MAIN ANALYSIS PIPELINE - ALL 7 MODULES\n",
    "# Following complete research methodology from data collection to bottleneck detection\n",
    "# =============================================================================\n",
    "def run_complete_research_methodology(data_source, time_interval_days=360):\n",
    "   \"\"\"\n",
    "   Run complete 7-module research methodology for stoppage pattern analysis\n",
    "   Following exact research paper approach adapted for stoppage analysis\n",
    "   UPDATED: Support for Excel files (.xlsx, .xls)\n",
    "   \n",
    "   Parameters:\n",
    "   - data_source: Excel file path (.xlsx/.xls), CSV file path, or DataFrame\n",
    "   - time_interval_days: Historical data period (default 30 days as in research)\n",
    "   \"\"\"\n",
    "   print(\"=\"*80)\n",
    "   print(\"MANUFACTURING STOPPAGE ANALYSIS - COMPLETE RESEARCH METHODOLOGY\")\n",
    "   print(\"Following 7-module CRISP-DM approach from research paper\")\n",
    "   print(\"Adapted from throughput bottleneck detection to stoppage pattern analysis\")\n",
    "   print(\"UPDATED: Supporting Excel file input (.xlsx, .xls)\")\n",
    "   print(\"=\"*80)\n",
    "   \n",
    "   # Initialize all modules\n",
    "   module1 = Module1_DataCollection()\n",
    "   module2 = Module2_MethodSelection()\n",
    "   module3 = Module3_DataPreprocessing()\n",
    "   module4 = Module4_DTWClustering()\n",
    "   module5 = Module5_ClusterGeneration()\n",
    "   module6 = Module6_RepresentativeTimeSeries()\n",
    "   module7 = Module7_BottleneckDetection()\n",
    "   \n",
    "   # MODULE 1: Data Collection\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 1: DATA COLLECTION FROM EXCEL\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   raw_data = module1.collect_event_log_data(data_source, time_interval_days)\n",
    "   \n",
    "   # MODULE 2: Method Selection\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 2: METHOD SELECTION\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   selected_method = module2.select_detection_method(raw_data)\n",
    "   \n",
    "   # MODULE 3: Data Preprocessing\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 3: DATA PREPROCESSING\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   processed_data = module3.preprocess_event_data(raw_data)\n",
    "   \n",
    "   # Generate time series matrix (research methodology)\n",
    "   time_series_matrix = module3._generate_time_series_features()\n",
    "   \n",
    "   # Create time series plots (research Figure 3 style)\n",
    "   if hasattr(module3, 'plot_time_series_like_research'):\n",
    "       module3.plot_time_series_like_research()\n",
    "   \n",
    "   # MODULE 4: DTW Clustering and Dendrogram Generation\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 4: DTW CLUSTERING AND DENDROGRAM\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   clustering_results = module4.apply_dtw_clustering(\n",
    "       processed_data, \n",
    "       time_series_matrix, \n",
    "       module3.clustering_features\n",
    "   )\n",
    "   \n",
    "   # MODULE 5: Cluster Computation and Generation\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 5: CLUSTER COMPUTATION AND GENERATION\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   processed_data_with_clusters = module5.generate_clusters(\n",
    "       processed_data, \n",
    "       clustering_results, \n",
    "       module4.linkage_matrix,\n",
    "       module3.time_series_data\n",
    "   )\n",
    "   \n",
    "   # MODULE 6: Representative Time Series Generation\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 6: REPRESENTATIVE TIME SERIES GENERATION\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   representative_time_series = module6.generate_representative_time_series(\n",
    "       time_series_matrix, \n",
    "       clustering_results\n",
    "   )\n",
    "   \n",
    "   # MODULE 7: Bottleneck Detection (Adapted to Stoppage Analysis)\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"STARTING MODULE 7: STOPPAGE PATTERN DETECTION\")\n",
    "   print(\"=\"*60)\n",
    "   \n",
    "   bottleneck_analysis = module7.detect_bottlenecks(\n",
    "       representative_time_series, \n",
    "       module6.cluster_assignments\n",
    "   )\n",
    "   \n",
    "   # Compile complete results\n",
    "   complete_results = {\n",
    "       'module1_data_collection': {\n",
    "           'raw_data': raw_data,\n",
    "           'domain_expert_input': module1.domain_expert_input,\n",
    "           'time_interval_days': time_interval_days,\n",
    "           'data_source_type': 'Excel' if isinstance(data_source, str) and data_source.endswith(('.xlsx', '.xls')) else 'Other'\n",
    "       },\n",
    "       'module2_method_selection': {\n",
    "           'selected_method': selected_method,\n",
    "           'method_justification': module2.method_justification,\n",
    "           'data_verification': module2.data_verification\n",
    "       },\n",
    "       'module3_preprocessing': {\n",
    "           'processed_data': processed_data,\n",
    "           'time_series_matrix': time_series_matrix,\n",
    "           'clustering_features': module3.clustering_features,\n",
    "           'preprocessing_steps': module3.preprocessing_steps\n",
    "       },\n",
    "       'module4_clustering': {\n",
    "           'clustering_results': clustering_results,\n",
    "           'distance_matrix': module4.distance_matrix,\n",
    "           'linkage_matrix': module4.linkage_matrix,\n",
    "           'dendrogram_data': module4.dendrogram_data\n",
    "       },\n",
    "       'module5_cluster_generation': {\n",
    "           'processed_data_with_clusters': processed_data_with_clusters,\n",
    "           'cluster_characteristics': module5.cluster_characteristics,\n",
    "           'cluster_validation': module5.cluster_validation,\n",
    "           'optimal_clusters': module5.optimal_clusters\n",
    "       },\n",
    "       'module6_representative_series': {\n",
    "           'representative_time_series': representative_time_series,\n",
    "           'cluster_assignments': module6.cluster_assignments\n",
    "       },\n",
    "       'module7_pattern_detection': {\n",
    "           'bottleneck_analysis': bottleneck_analysis,\n",
    "           'domain_expert_interpretation': module7.domain_expert_interpretation,\n",
    "           'feedback_recommendations': module7.feedback_recommendations\n",
    "       }\n",
    "   }\n",
    "   \n",
    "   # Print final summary\n",
    "   print(\"\\n\" + \"=\"*80)\n",
    "   print(\"RESEARCH METHODOLOGY EXECUTION COMPLETE\")\n",
    "   print(\"=\"*80)\n",
    "   \n",
    "   print_methodology_summary(complete_results)\n",
    "   \n",
    "   return complete_results\n",
    "\n",
    "def print_methodology_summary(results):\n",
    "   \"\"\"\n",
    "   Print comprehensive summary of research methodology execution\n",
    "   \"\"\"\n",
    "   print(\"\\n=== RESEARCH METHODOLOGY EXECUTION SUMMARY ===\")\n",
    "   \n",
    "   # Module 1 Summary\n",
    "   module1 = results['module1_data_collection']\n",
    "   print(f\"\\n✓ MODULE 1 - Data Collection:\")\n",
    "   print(f\"  • Events collected: {len(module1['raw_data'])}\")\n",
    "   print(f\"  • Time period: {module1['time_interval_days']} days\")\n",
    "   print(f\"  • Lines analyzed: {module1['raw_data']['Line'].nunique()}\")\n",
    "   \n",
    "   # Module 2 Summary\n",
    "   module2 = results['module2_method_selection']\n",
    "   print(f\"\\n✓ MODULE 2 - Method Selection:\")\n",
    "   print(f\"  • Selected method: {module2['selected_method']}\")\n",
    "   print(f\"  • Data verification: {'Passed' if all(module2['data_verification'].values()) else 'Issues detected'}\")\n",
    "   \n",
    "   # Module 3 Summary\n",
    "   module3 = results['module3_preprocessing']\n",
    "   print(f\"\\n✓ MODULE 3 - Data Preprocessing:\")\n",
    "   print(f\"  • Time series matrix: {module3['time_series_matrix'].shape}\")\n",
    "   print(f\"  • Clustering features: {len(module3['clustering_features'])}\")\n",
    "   print(f\"  • Processing steps: {len(module3['preprocessing_steps'])}\")\n",
    "   \n",
    "   # Module 4 Summary\n",
    "   module4 = results['module4_clustering']\n",
    "   ahc_results = module4['clustering_results'].get('AHC_Research', {})\n",
    "   print(f\"\\n✓ MODULE 4 - DTW Clustering:\")\n",
    "   print(f\"  • Distance matrix: {module4['distance_matrix'].shape}\")\n",
    "   print(f\"  • AHC clusters: {ahc_results.get('n_clusters', 'N/A')}\")\n",
    "   print(f\"  • Linkage method: {ahc_results.get('linkage_method', 'N/A')}\")\n",
    "   \n",
    "   # Module 5 Summary\n",
    "   module5 = results['module5_cluster_generation']\n",
    "   print(f\"\\n✓ MODULE 5 - Cluster Generation:\")\n",
    "   print(f\"  • Optimal clusters: {module5['optimal_clusters']}\")\n",
    "   print(f\"  • Validation metrics available: {'Yes' if module5['cluster_validation'] else 'No'}\")\n",
    "   \n",
    "   # Module 6 Summary\n",
    "   module6 = results['module6_representative_series']\n",
    "   print(f\"\\n✓ MODULE 6 - Representative Time Series:\")\n",
    "   print(f\"  • Representative series: {len(module6['representative_time_series'])}\")\n",
    "   print(f\"  • Cluster assignments: {len(module6['cluster_assignments'])}\")\n",
    "   \n",
    "   # Module 7 Summary\n",
    "   module7 = results['module7_pattern_detection']\n",
    "   feedback = module7['feedback_recommendations']\n",
    "   print(f\"\\n✓ MODULE 7 - Pattern Detection:\")\n",
    "   print(f\"  • Patterns analyzed: {len(module7['domain_expert_interpretation'])}\")\n",
    "   print(f\"  • Feedback needed: {'Yes' if feedback['feedback_needed'] else 'No'}\")\n",
    "   \n",
    "   # Overall Assessment\n",
    "   print(f\"\\n=== OVERALL RESEARCH METHODOLOGY ASSESSMENT ===\")\n",
    "   print(f\"✓ All 7 modules executed successfully\")\n",
    "   print(f\"✓ Following exact research paper methodology\")\n",
    "   print(f\"✓ Adapted from throughput bottleneck to stoppage pattern analysis\")\n",
    "   print(f\"✓ HDBSCAN comparison added as enhancement\")\n",
    "   \n",
    "   if feedback['feedback_needed']:\n",
    "       print(f\"⚠ Feedback recommended: Consider re-running Modules 5-6-7\")\n",
    "   else:\n",
    "       print(f\"✓ Analysis complete - no feedback loop required\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED VISUALIZATION AND INTERPRETATION\n",
    "# =============================================================================\n",
    "def plot_individual_time_series_research_style(time_series_matrix, ax):\n",
    "   \"\"\"\n",
    "   Plot individual stoppage reason time series following research Figure 3 style\n",
    "   ADAPTED: Shows stoppage reason patterns instead of machine patterns\n",
    "   \"\"\"\n",
    "   production_runs = range(1, len(time_series_matrix) + 1)\n",
    "   colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "   \n",
    "   for i, stoppage_reason in enumerate(time_series_matrix.columns):\n",
    "       color = colors[i % len(colors)]\n",
    "       series = time_series_matrix[stoppage_reason].values\n",
    "       ax.plot(production_runs, series, color=color, linewidth=1.5, \n",
    "              label=stoppage_reason[:20] + '...' if len(stoppage_reason) > 20 else stoppage_reason, \n",
    "              alpha=0.8)\n",
    "   \n",
    "   ax.set_xlabel('Production Run', fontsize=11)\n",
    "   ax.set_ylabel('Stoppage Impact (%)', fontsize=11)\n",
    "   ax.set_title('Individual Stoppage Reason Time Series\\n(Research Figure 3 Style - Adapted)', fontsize=12, fontweight='bold')\n",
    "   ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "   ax.grid(True, alpha=0.3)\n",
    "   \n",
    "   # Add research-style annotation (adapted)\n",
    "   ax.text(0.02, 0.98, 'Complex stoppage patterns\\nrequire ML analysis', \n",
    "          transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "          bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "def plot_dendrogram_research_style(clustering_module, ax):\n",
    "   \"\"\"\n",
    "   Plot dendrogram following research Figure 5/9 style\n",
    "   ADAPTED: Shows stoppage reason relationships instead of machine relationships\n",
    "   \"\"\"\n",
    "   if hasattr(clustering_module, 'linkage_matrix') and clustering_module['linkage_matrix'] is not None:\n",
    "       # Get stoppage reason names\n",
    "       ahc_results = clustering_module['clustering_results'].get('AHC_Research', {})\n",
    "       stoppage_reason_names = ahc_results.get('stoppage_reason_names', [f'R{i+1}' for i in range(len(clustering_module['linkage_matrix'])+1)])\n",
    "       \n",
    "       # Truncate long names for better visualization\n",
    "       display_names = [name[:15] + '...' if len(name) > 15 else name for name in stoppage_reason_names]\n",
    "       \n",
    "       # Create dendrogram\n",
    "       dendrogram_data = dendrogram(\n",
    "           clustering_module['linkage_matrix'],\n",
    "           labels=display_names,\n",
    "           ax=ax,\n",
    "           orientation='top',\n",
    "           distance_sort='descending'\n",
    "       )\n",
    "       \n",
    "       # Add cluster highlighting (research style)\n",
    "       n_clusters = ahc_results.get('n_clusters', 2)\n",
    "       if n_clusters > 1:\n",
    "           cut_height = clustering_module['linkage_matrix'][-(n_clusters-1), 2]\n",
    "           ax.axhline(y=cut_height, color='red', linestyle='--', linewidth=2, \n",
    "                     label=f'{n_clusters} clusters')\n",
    "           \n",
    "           # Add cluster labels (research Figure 9 style)\n",
    "           cluster_positions = np.linspace(0.1, 0.9, n_clusters)\n",
    "           for i in range(min(n_clusters, len(cluster_positions))):\n",
    "               ax.text(cluster_positions[i], 0.9, f'Cluster {i+1}', \n",
    "                      transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "                      bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "       \n",
    "       ax.set_title('Dendrogram - Stoppage Reason Clusters\\n(Research Figure 5/9 Style - Adapted)', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "       ax.set_ylabel('DTW Distance', fontsize=11)\n",
    "       ax.tick_params(axis='x', rotation=45)\n",
    "       ax.legend()\n",
    "       \n",
    "       # Add adaptation note\n",
    "       ax.text(0.02, 0.02, 'ADAPTATION:\\nStoppage reasons\\ninstead of machines', \n",
    "              transform=ax.transAxes, fontsize=9, \n",
    "              bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "   else:\n",
    "       ax.text(0.5, 0.5, 'Dendrogram data\\nnot available', \n",
    "              ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "       ax.set_title('Dendrogram (Not Available)', fontsize=12)\n",
    "\n",
    "def plot_representative_series_research_style(representative_series, ax):\n",
    "   \"\"\"\n",
    "   Plot representative time series following research Figure 8 style\n",
    "   ADAPTED: For stoppage reason clusters instead of machine clusters\n",
    "   \"\"\"\n",
    "   if not representative_series:\n",
    "       ax.text(0.5, 0.5, 'Representative series\\nnot available', \n",
    "              ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "       ax.set_title('Representative Time Series (Not Available)', fontsize=12)\n",
    "       return\n",
    "   \n",
    "   # Get production run range\n",
    "   n_runs = len(list(representative_series.values())[0]['series'])\n",
    "   production_runs = range(1, n_runs + 1)\n",
    "   \n",
    "   # Colors matching research style\n",
    "   colors = ['red', 'blue', 'green', 'orange']\n",
    "   \n",
    "   for i, (cluster_name, cluster_data) in enumerate(representative_series.items()):\n",
    "       series = cluster_data['series']\n",
    "       color = colors[i % len(colors)]\n",
    "       \n",
    "       # Plot with research styling\n",
    "       ax.plot(production_runs, series, color=color, linewidth=3, \n",
    "              label=f'{cluster_name}', marker='o', markersize=4)\n",
    "       \n",
    "       # Add confidence interval\n",
    "       if 'std_deviation' in cluster_data:\n",
    "           std_dev = cluster_data['std_deviation']\n",
    "           ax.fill_between(production_runs, series - std_dev, series + std_dev, \n",
    "                          color=color, alpha=0.2)\n",
    "   \n",
    "   ax.set_xlabel('Production Run', fontsize=11)\n",
    "   ax.set_ylabel('Stoppage Impact (%)', fontsize=11)\n",
    "   ax.set_title('Representative Time Series - Stoppage Reason Clusters\\n(Research Figure 8 Style - Adapted)', \n",
    "               fontsize=12, fontweight='bold')\n",
    "   ax.legend()\n",
    "   ax.grid(True, alpha=0.3)\n",
    "   \n",
    "   # Add research-style interpretation note (adapted)\n",
    "   if len(representative_series) > 0:\n",
    "       max_cluster = max(representative_series.items(), key=lambda x: np.mean(x[1]['series']))\n",
    "       ax.text(0.02, 0.98, f'Highest impact pattern:\\n{max_cluster[0]}', \n",
    "              transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "def plot_validation_metrics(validation_results, ax):\n",
    "   \"\"\"\n",
    "   Plot validation metrics comparison\n",
    "   \"\"\"\n",
    "   methods = []\n",
    "   silhouette_scores = []\n",
    "   \n",
    "   for method, metrics in validation_results.items():\n",
    "       if method != 'Domain_Expert' and isinstance(metrics, dict) and 'silhouette_score' in metrics:\n",
    "           methods.append(method)\n",
    "           silhouette_scores.append(metrics['silhouette_score'])\n",
    "   \n",
    "   if methods:\n",
    "       bars = ax.bar(methods, silhouette_scores, color=['skyblue', 'lightgreen'][:len(methods)])\n",
    "       ax.set_ylabel('Silhouette Score', fontsize=11)\n",
    "       ax.set_title('Clustering Method Validation\\n(Higher is Better)', fontsize=12, fontweight='bold')\n",
    "       ax.set_ylim(0, 1)\n",
    "       \n",
    "       # Add value labels on bars\n",
    "       for bar, score in zip(bars, silhouette_scores):\n",
    "           height = bar.get_height()\n",
    "           ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                  f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "       \n",
    "       # Add interpretation threshold\n",
    "       ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Good threshold (0.5)')\n",
    "       ax.legend()\n",
    "   else:\n",
    "       ax.text(0.5, 0.5, 'Validation metrics\\nnot available', \n",
    "              ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "       ax.set_title('Validation Metrics (Not Available)', fontsize=12)\n",
    "\n",
    "def plot_stoppage_pattern_analysis(processed_data, ax):\n",
    "   \"\"\"\n",
    "   Plot stoppage pattern analysis\n",
    "   \"\"\"\n",
    "   if 'Event_Category' in processed_data.columns:\n",
    "       # Analyze patterns by category\n",
    "       category_duration = processed_data.groupby('Event_Category')['Stoppage_Duration_Minutes'].agg(['count', 'mean'])\n",
    "       \n",
    "       # Create bubble chart\n",
    "       categories = category_duration.index\n",
    "       x_pos = range(len(categories))\n",
    "       counts = category_duration['count']\n",
    "       avg_durations = category_duration['mean']\n",
    "       \n",
    "       # Normalize bubble sizes\n",
    "       max_count = counts.max()\n",
    "       bubble_sizes = (counts / max_count) * 1000 + 100\n",
    "       \n",
    "       scatter = ax.scatter(x_pos, avg_durations, s=bubble_sizes, alpha=0.6, \n",
    "                          c=range(len(categories)), cmap='viridis')\n",
    "       \n",
    "       ax.set_xticks(x_pos)\n",
    "       ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "       ax.set_ylabel('Avg Duration (min)', fontsize=11)\n",
    "       ax.set_title('Stoppage Pattern Analysis\\n(Bubble size = frequency)', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "       \n",
    "       # Add colorbar\n",
    "       plt.colorbar(scatter, ax=ax, label='Category Index')\n",
    "       \n",
    "   else:\n",
    "       ax.text(0.5, 0.5, 'Stoppage pattern\\ndata not available', \n",
    "              ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "       ax.set_title('Stoppage Pattern Analysis (Not Available)', fontsize=12)\n",
    "\n",
    "def plot_method_comparison(clustering_results, ax):\n",
    "   \"\"\"\n",
    "   Plot HDBSCAN vs AHC comparison\n",
    "   \"\"\"\n",
    "   methods = []\n",
    "   cluster_counts = []\n",
    "   colors = []\n",
    "   \n",
    "   if 'AHC_Research' in clustering_results:\n",
    "       methods.append('AHC\\n(Research)')\n",
    "       cluster_counts.append(clustering_results['AHC_Research']['n_clusters'])\n",
    "       colors.append('skyblue')\n",
    "   \n",
    "   if 'HDBSCAN_Comparison' in clustering_results:\n",
    "       methods.append('HDBSCAN\\n(Comparison)')\n",
    "       cluster_counts.append(clustering_results['HDBSCAN_Comparison']['n_clusters'])\n",
    "       colors.append('lightcoral')\n",
    "   \n",
    "   if methods:\n",
    "       bars = ax.bar(methods, cluster_counts, color=colors)\n",
    "       ax.set_ylabel('Number of Clusters', fontsize=11)\n",
    "       ax.set_title('Method Comparison\\nHDBSCAN vs AHC', fontsize=12, fontweight='bold')\n",
    "       \n",
    "       # Add value labels\n",
    "       for bar, count in zip(bars, cluster_counts):\n",
    "           height = bar.get_height()\n",
    "           ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                  str(count), ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "       \n",
    "       # Add noise information for HDBSCAN\n",
    "       if 'HDBSCAN_Comparison' in clustering_results:\n",
    "           noise_count = clustering_results['HDBSCAN_Comparison'].get('n_noise', 0)\n",
    "           ax.text(0.02, 0.98, f'HDBSCAN noise points: {noise_count}', \n",
    "                  transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "   else:\n",
    "       ax.text(0.5, 0.5, 'Clustering results\\nnot available', \n",
    "              ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "       ax.set_title('Method Comparison (Not Available)', fontsize=12)\n",
    "\n",
    "def generate_research_report(results):\n",
    "   \"\"\"\n",
    "   Generate comprehensive research report following paper style\n",
    "   ADAPTED: For stoppage reason analysis instead of machine analysis\n",
    "   \"\"\"\n",
    "   print(\"\\n\" + \"=\"*80)\n",
    "   print(\"COMPREHENSIVE RESEARCH REPORT\")\n",
    "   print(\"HDBSCAN vs AHC for Manufacturing Stoppage Analysis\")\n",
    "   print(\"Following Research Paper Methodology - Adapted for Stoppage Reasons\")\n",
    "   print(\"=\"*80)\n",
    "   \n",
    "   # Executive Summary\n",
    "   print(\"\\n=== EXECUTIVE SUMMARY ===\")\n",
    "   \n",
    "   # Get key results\n",
    "   module7 = results['module7_pattern_detection']\n",
    "   interpretations = module7['domain_expert_interpretation']\n",
    "   \n",
    "   if interpretations:\n",
    "       primary_cluster = None\n",
    "       for cluster_name, interp in interpretations.items():\n",
    "           if interp['ranking'] == 'Primary Problem Cluster':\n",
    "               primary_cluster = interp\n",
    "               break\n",
    "       \n",
    "       if primary_cluster:\n",
    "           print(f\"• Primary problematic pattern identified: {primary_cluster['stoppage_reasons']}\")\n",
    "           print(f\"• Average impact rate: {primary_cluster['avg_impact_rate']:.2f}%\")\n",
    "           print(f\"• Recommended action: {primary_cluster['recommendation']}\")\n",
    "       \n",
    "       # Method comparison\n",
    "       ahc_clusters = results['module4_clustering']['clustering_results'].get('AHC_Research', {}).get('n_clusters', 'N/A')\n",
    "       hdbscan_clusters = results['module4_clustering']['clustering_results'].get('HDBSCAN_Comparison', {}).get('n_clusters', 'N/A')\n",
    "       \n",
    "       print(f\"• AHC identified {ahc_clusters} stoppage reason clusters using research methodology\")\n",
    "       print(f\"• HDBSCAN identified {hdbscan_clusters} clusters as comparison\")\n",
    "       \n",
    "       # Validation summary\n",
    "       validation = results['module5_cluster_generation']['cluster_validation']\n",
    "       if 'AHC' in validation and 'silhouette_score' in validation['AHC']:\n",
    "           ahc_score = validation['AHC']['silhouette_score']\n",
    "           print(f\"• AHC silhouette score: {ahc_score:.3f}\")\n",
    "       \n",
    "       if 'HDBSCAN' in validation and 'silhouette_score' in validation['HDBSCAN']:\n",
    "           hdbscan_score = validation['HDBSCAN']['silhouette_score']\n",
    "           print(f\"• HDBSCAN silhouette score: {hdbscan_score:.3f}\")\n",
    "   \n",
    "   # Research Methodology Compliance\n",
    "   print(\"\\n=== RESEARCH METHODOLOGY COMPLIANCE ===\")\n",
    "   print(\"✓ Module 1: Event log data collection (Excel/CSV support)\")\n",
    "   print(\"✓ Module 2: Method selection with domain expert input\")\n",
    "   print(\"✓ Module 3: Data preprocessing with event classification\")\n",
    "   print(\"✓ Module 4: DTW distance calculation and hierarchical clustering\")\n",
    "   print(\"✓ Module 5: Cluster generation with elbow method validation\")\n",
    "   print(\"✓ Module 6: Representative time series generation\")\n",
    "   print(\"✓ Module 7: Pattern detection with visual analysis\")\n",
    "   print(\"✓ Enhancement: HDBSCAN comparison for density-based clustering\")\n",
    "   print(\"✓ ADAPTATION: Focus on stoppage reasons instead of machines\")\n",
    "   \n",
    "   # Recommendations\n",
    "   print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "   recommendations = module7['bottleneck_analysis'].get('recommendations', {})\n",
    "   for category, recs in recommendations.items():\n",
    "       if recs:\n",
    "           print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "           for rec in recs[:3]:  # Top 3 recommendations\n",
    "               print(f\"  • {rec}\")\n",
    "   \n",
    "   # Future Work\n",
    "   print(\"\\n=== FUTURE WORK ===\")\n",
    "   feedback = module7['feedback_recommendations']\n",
    "   if feedback['feedback_needed']:\n",
    "       print(\"• Consider re-evaluating cluster parameters based on feedback analysis\")\n",
    "       for action in feedback['recommended_actions'][:2]:\n",
    "           print(f\"  - {action}\")\n",
    "   else:\n",
    "       print(\"• Implement real-time monitoring based on identified stoppage patterns\")\n",
    "       print(\"• Extend analysis to longer time periods for validation\")\n",
    "       print(\"• Apply methodology to other production lines\")\n",
    "       print(\"• Develop stoppage-reason-specific maintenance strategies\")\n",
    "   \n",
    "   print(\"\\n\" + \"=\"*80)\n",
    "   print(\"REPORT COMPLETE - STOPPAGE REASON FOCUS\")\n",
    "   print(\"=\"*80)\n",
    "\n",
    "def create_comprehensive_visualizations(results):\n",
    "   \"\"\"\n",
    "   Create comprehensive visualizations following research paper style\n",
    "   \"\"\"\n",
    "   print(\"\\n=== CREATING COMPREHENSIVE VISUALIZATIONS ===\")\n",
    "   print(\"Following research paper visualization style\")\n",
    "   \n",
    "   # Get data\n",
    "   time_series_matrix = results['module3_preprocessing']['time_series_matrix']\n",
    "   clustering_results = results['module4_clustering']['clustering_results']\n",
    "   representative_series = results['module6_representative_series']['representative_time_series']\n",
    "   processed_data = results['module5_cluster_generation']['processed_data_with_clusters']\n",
    "   \n",
    "   # Create multi-panel visualization following research figures\n",
    "   fig = plt.figure(figsize=(20, 16))\n",
    "   \n",
    "   # Panel 1: Individual stoppage reason time series (Research Figure 3 style)\n",
    "   ax1 = plt.subplot(3, 2, 1)\n",
    "   plot_individual_time_series_research_style(time_series_matrix, ax1)\n",
    "   \n",
    "   # Panel 2: Dendrogram (Research Figure 5/9 style)\n",
    "   ax2 = plt.subplot(3, 2, 2)\n",
    "   plot_dendrogram_research_style(results['module4_clustering'], ax2)\n",
    "   \n",
    "   # Panel 3: Representative time series (Research Figure 8 style)\n",
    "   ax3 = plt.subplot(3, 2, 3)\n",
    "   plot_representative_series_research_style(representative_series, ax3)\n",
    "   \n",
    "   # Panel 4: Cluster validation metrics\n",
    "   ax4 = plt.subplot(3, 2, 4)\n",
    "   plot_validation_metrics(results['module5_cluster_generation']['cluster_validation'], ax4)\n",
    "   \n",
    "   # Panel 5: Stoppage pattern analysis\n",
    "   ax5 = plt.subplot(3, 2, 5)\n",
    "   plot_stoppage_pattern_analysis(processed_data, ax5)\n",
    "   \n",
    "   # Panel 6: HDBSCAN vs AHC comparison\n",
    "   ax6 = plt.subplot(3, 2, 6)\n",
    "   plot_method_comparison(clustering_results, ax6)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.suptitle('Complete Research Methodology Results\\nHDBSCAN vs AHC for Manufacturing Stoppage Analysis', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "   plt.show()\n",
    "   \n",
    "   return fig\n",
    "\n",
    "# Helper function for Excel file analysis\n",
    "def analyze_excel_file(file_path):\n",
    "   \"\"\"\n",
    "   Quick analysis function for Excel files\n",
    "   \"\"\"\n",
    "   print(f\"\\n=== QUICK EXCEL FILE ANALYSIS ===\")\n",
    "   print(f\"File: {file_path}\")\n",
    "   \n",
    "   try:\n",
    "       # Read first few rows to understand structure\n",
    "       df_preview = pd.read_excel(file_path, nrows=5)\n",
    "       print(f\"\\nFile structure preview:\")\n",
    "       print(f\"Columns: {list(df_preview.columns)}\")\n",
    "       print(f\"Shape: {df_preview.shape}\")\n",
    "       print(f\"\\nFirst few rows:\")\n",
    "       print(df_preview.to_string())\n",
    "       \n",
    "       # Run full analysis\n",
    "       print(f\"\\n=== RUNNING FULL ANALYSIS ===\")\n",
    "       results = run_complete_research_methodology(file_path)\n",
    "       \n",
    "       return results\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"Error analyzing Excel file: {e}\")\n",
    "       return None\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "   print(\"Manufacturing Stoppage Analysis - Complete Research Methodology\")\n",
    "   print(\"=\"*70)\n",
    "   print(\"\\nThis notebook implements the complete 7-module research methodology\")\n",
    "   print(\"from the paper: 'Generic hierarchical clustering approach to throughput\")\n",
    "   print(\"bottleneck detection' adapted for stoppage pattern analysis.\")\n",
    "   print(\"\\nUPDATED: Now supports Excel file input (.xlsx, .xls)\")\n",
    "   print(\"\\nTo execute the complete analysis:\")\n",
    "   print(\"\\n1. With your Excel data:\")\n",
    "   print(\"   results = run_complete_research_methodology('your_data.xlsx')\")\n",
    "   print(\"\\n2. With your CSV data:\")\n",
    "   print(\"   results = run_complete_research_methodology('your_data.csv')\")\n",
    "   print(\"\\n3. With sample data:\")\n",
    "   print(\"   results = run_complete_research_methodology(None)\")\n",
    "   print(\"\\n4. Create visualizations:\")\n",
    "   print(\"   create_comprehensive_visualizations(results)\")\n",
    "   print(\"\\n5. Generate report:\")\n",
    "   print(\"   generate_research_report(results)\")\n",
    "   print(\"\\nExpected Excel columns:\")\n",
    "   print(\"- Line (or Production Line, Machine)\")\n",
    "   print(\"- Stoppage Reason (or Reason, Downtime Reason)\")\n",
    "   print(\"- Start Datetime (or Start Time)\")\n",
    "   print(\"- End Datetime (or End Time)\")\n",
    "   print(\"- Shift Id (or Shift)\")\n",
    "   print(\"\\nNote: Column names are automatically standardized\")\n",
    "   print(\"\\n\" + \"=\"*70)\n",
    "   print(\"Ready to execute complete research methodology with Excel support!\")\n",
    "   \n",
    "   # Set the file path\n",
    "   file_path = r'data\\Line2.xlsx'\n",
    "   \n",
    "   # Execute analysis with your Excel file\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EXECUTING: Running with Excel file\")\n",
    "   print(\"=\"*50)\n",
    "   \n",
    "   results = run_complete_research_methodology(file_path)\n",
    "   create_comprehensive_visualizations(results)\n",
    "   generate_research_report(results)\n",
    "   \n",
    "   print(\"\\nAnalysis complete! Results available in 'results' variable.\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54097a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
